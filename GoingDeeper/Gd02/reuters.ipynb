{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5e3a0e48-2600-4c1f-bf3a-f3fd86021ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7bc74ac-37e6-4204-8fb9-46f040b6b8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-01 07:53:20.786234: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-09-01 07:53:20.855346: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-09-01 07:53:22.548005: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.datasets import reuters\n",
    "import pandas as pd\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "291ca723-3a28-4f77-9370-7d97f534487b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "# word_index = reuters.get_word_index(path=\"reuters_word_index.json\")\n",
    "#!wget https://storage.googleapis.com/tensorflow/tf-keras-datasets/reuters_word_index.json\n",
    "\n",
    "import json\n",
    "\n",
    "# 1. 파일 열기\n",
    "with open('reuters_word_index.json', 'r', encoding='utf-8') as f:\n",
    "    # 2. JSON 로드\n",
    "    word_index = json.load(f)\n",
    "\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3aa9d31d-28a2-4eec-b378-afdf3880546c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5abe535a-e6fa-4fd9-8582-1830253244fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index['it']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5cbdb908-59cd-4bc0-b027-224c3d3be04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "index_to_word = { index+3 : word for word, index in word_index.items() }\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d095ba8b-3441-4012-ae07-ea06ef712923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the\n",
      "it\n"
     ]
    }
   ],
   "source": [
    "print(index_to_word[4])\n",
    "print(index_to_word[16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9e78fa5-67db-4bf3-8292-eb75a15782e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "# index_to_word에 숫자 0은 <pad>, 숫자 1은 <sos>, 숫자 2는 <unk>를 넣어줍니다.\n",
    "for index, token in enumerate((\"<pad>\", \"<sos>\", \"<unk>\")):\n",
    "  index_to_word[index]=token\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87366708-2c47-4792-b854-04c0fab032c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, 10000, 5000]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_words = [None, 10000, 5000] # 단어 개수별 실험 준비\n",
    "num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e952d63-027d-4463-9cfd-79296d9a1446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d52e9b4-b8a8-470f-8ce0-afc1e44dda02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB #다항분포 나이브 베이즈 모델\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score #정확도 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7347c2c1-94cc-4923-a917-0efbe8f9d841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=3\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print('=3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de08437f-2f8a-4ecf-a8ec-8c91ada68948",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** 모든 단어 *****\n",
      "(8982, 26506)\n",
      "(8982, 26506)\n",
      "--- (1) 나이브 베이즈 분류기 ---\n",
      "정확도: 0.5997328584149599\n",
      "F1 Score: 0.06767886443611608\n",
      "--- (2) Complement Naive Bayes Classifier(CNB) ---\n",
      "정확도: 0.7649154051647373\n",
      "F1 Score: 0.4639854358945848\n",
      "--- (3) 로지스틱 회귀(Logistic Regression) ---\n",
      "정확도: 0.811219946571683\n",
      "F1 Score: 0.6629074440453098\n",
      "--- (4) 선형 서포트 벡터 머신(Linear Support Vector Machine) ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.12/site-packages/sklearn/svm/_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "정확도: 0.7929652715939448\n",
      "F1 Score: 0.6694322208704948\n",
      "--- (5) 결정 트리(Decision Tree) ---\n",
      "정확도: 0.6211041852181657\n",
      "F1 Score: 0.1545005540480078\n",
      "--- (6) 랜덤 포레스트(Random Forest) ---\n",
      "정확도: 0.6544968833481746\n",
      "F1 Score: 0.2794631711818573\n",
      "--- (7) 그래디언트 부스팅 트리(GradientBoostingClassifier) ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 86\u001b[39m\n\u001b[32m     84\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m--- (7) 그래디언트 부스팅 트리(GradientBoostingClassifier) ---\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     85\u001b[39m grbt = GradientBoostingClassifier(random_state=\u001b[32m0\u001b[39m) \u001b[38;5;66;03m# verbose=3\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m \u001b[43mgrbt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtfidfv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     87\u001b[39m predicted = grbt.predict(tfidfv_test) \u001b[38;5;66;03m#테스트 데이터에 대한 예측\u001b[39;00m\n\u001b[32m     88\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m정확도:\u001b[39m\u001b[33m\"\u001b[39m, accuracy_score(y_test, predicted)) \u001b[38;5;66;03m#예측값과 실제값 비교\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/sklearn/base.py:1363\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1356\u001b[39m     estimator._validate_params()\n\u001b[32m   1358\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1359\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1360\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1361\u001b[39m     )\n\u001b[32m   1362\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1363\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/sklearn/ensemble/_gb.py:787\u001b[39m, in \u001b[36mBaseGradientBoosting.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, monitor)\u001b[39m\n\u001b[32m    784\u001b[39m     \u001b[38;5;28mself\u001b[39m._resize_state()\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m n_stages = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_stages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbegin_at_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[32m    801\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_stages != \u001b[38;5;28mself\u001b[39m.estimators_.shape[\u001b[32m0\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/sklearn/ensemble/_gb.py:883\u001b[39m, in \u001b[36mBaseGradientBoosting._fit_stages\u001b[39m\u001b[34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[39m\n\u001b[32m    876\u001b[39m         initial_loss = factor * \u001b[38;5;28mself\u001b[39m._loss(\n\u001b[32m    877\u001b[39m             y_true=y_oob_masked,\n\u001b[32m    878\u001b[39m             raw_prediction=raw_predictions[~sample_mask],\n\u001b[32m    879\u001b[39m             sample_weight=sample_weight_oob_masked,\n\u001b[32m    880\u001b[39m         )\n\u001b[32m    882\u001b[39m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m883\u001b[39m raw_predictions = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_stage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    886\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    887\u001b[39m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    888\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    889\u001b[39m \u001b[43m    \u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    890\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    891\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_csc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_csc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    892\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX_csr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_csr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    893\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    895\u001b[39m \u001b[38;5;66;03m# track loss\u001b[39;00m\n\u001b[32m    896\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/sklearn/ensemble/_gb.py:489\u001b[39m, in \u001b[36mBaseGradientBoosting._fit_stage\u001b[39m\u001b[34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[39m\n\u001b[32m    486\u001b[39m     sample_weight = sample_weight * sample_mask.astype(np.float64)\n\u001b[32m    488\u001b[39m X = X_csc \u001b[38;5;28;01mif\u001b[39;00m X_csc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m \u001b[43mtree\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_g_view\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m    491\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[38;5;66;03m# update tree leaves\u001b[39;00m\n\u001b[32m    494\u001b[39m X_for_tree_update = X_csr \u001b[38;5;28;01mif\u001b[39;00m X_csr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/sklearn/base.py:1363\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1356\u001b[39m     estimator._validate_params()\n\u001b[32m   1358\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1359\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1360\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1361\u001b[39m     )\n\u001b[32m   1362\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1363\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/sklearn/tree/_classes.py:1404\u001b[39m, in \u001b[36mDecisionTreeRegressor.fit\u001b[39m\u001b[34m(self, X, y, sample_weight, check_input)\u001b[39m\n\u001b[32m   1374\u001b[39m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   1375\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight=\u001b[38;5;28;01mNone\u001b[39;00m, check_input=\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m   1376\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[32m   1377\u001b[39m \n\u001b[32m   1378\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1401\u001b[39m \u001b[33;03m        Fitted estimator.\u001b[39;00m\n\u001b[32m   1402\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1404\u001b[39m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1405\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1406\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1407\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1408\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1409\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1410\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/lib/python3.12/site-packages/sklearn/tree/_classes.py:472\u001b[39m, in \u001b[36mBaseDecisionTree._fit\u001b[39m\u001b[34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    462\u001b[39m     builder = BestFirstTreeBuilder(\n\u001b[32m    463\u001b[39m         splitter,\n\u001b[32m    464\u001b[39m         min_samples_split,\n\u001b[32m   (...)\u001b[39m\u001b[32m    469\u001b[39m         \u001b[38;5;28mself\u001b[39m.min_impurity_decrease,\n\u001b[32m    470\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m472\u001b[39m \u001b[43mbuilder\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    474\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.n_outputs_ == \u001b[32m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    475\u001b[39m     \u001b[38;5;28mself\u001b[39m.n_classes_ = \u001b[38;5;28mself\u001b[39m.n_classes_[\u001b[32m0\u001b[39m]\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "for NUM in num_words:\n",
    "    if NUM == 10000:\n",
    "        print('***** 10,000개 단어 *****')\n",
    "    elif NUM == 5000:\n",
    "        print('***** 5,000개 단어 *****')\n",
    "    else:\n",
    "        print('***** 모든 단어 *****')\n",
    "    \n",
    "    (x_train, y_train), (x_test, y_test) = reuters.load_data(num_words=NUM, test_split=0.2) # 데이터 직접 가져옴\n",
    "\n",
    "    decoded = []\n",
    "    for i in range(len(x_train)):\n",
    "        t = ' '.join([index_to_word[index] for index in x_train[i]])\n",
    "        decoded.append(t)\n",
    "    \n",
    "    x_train = decoded\n",
    "\n",
    "    decoded = []\n",
    "    for i in range(len(x_test)):\n",
    "        t = ' '.join([index_to_word[index] for index in x_test[i]])\n",
    "        decoded.append(t)\n",
    "    \n",
    "    x_test = decoded\n",
    "\n",
    "    dtmvector = CountVectorizer()\n",
    "    x_train_dtm = dtmvector.fit_transform(x_train)\n",
    "    print(x_train_dtm.shape)\n",
    "\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    tfidfv = tfidf_transformer.fit_transform(x_train_dtm)\n",
    "    print(tfidfv.shape)\n",
    "\n",
    "    \n",
    "    print(\"--- (1) 나이브 베이즈 분류기 ---\")\n",
    "    model = MultinomialNB()\n",
    "    model.fit(tfidfv, y_train)\n",
    "    x_test_dtm = dtmvector.transform(x_test) #테스트 데이터를 DTM으로 변환\n",
    "    tfidfv_test = tfidf_transformer.transform(x_test_dtm) #DTM을 TF-IDF 행렬로 변환\n",
    "    \n",
    "    predicted = model.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "    print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "    print(\"F1 Score:\", f1_score(y_test, predicted, average='macro'))\n",
    "\n",
    "    # model.predict(tfidfv_test[3])\n",
    "\n",
    "    # ## 5. F1-Score, Confusion Matrix\n",
    "    # print(classification_report(y_test, model.predict(tfidfv_test), zero_division=0))\n",
    "\n",
    "    print(\"--- (2) Complement Naive Bayes Classifier(CNB) ---\")\n",
    "    cb = ComplementNB()\n",
    "    cb.fit(tfidfv, y_train)\n",
    "    predicted = cb.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "    print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "    print(\"F1 Score:\", f1_score(y_test, predicted, average='macro'))\n",
    "\n",
    "    print(\"--- (3) 로지스틱 회귀(Logistic Regression) ---\")\n",
    "    lr = LogisticRegression(C=10000, penalty='l2', max_iter=3000)\n",
    "    lr.fit(tfidfv, y_train)\n",
    "    predicted = lr.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "    print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "    print(\"F1 Score:\", f1_score(y_test, predicted, average='macro'))\n",
    "\n",
    "    print(\"--- (4) 선형 서포트 벡터 머신(Linear Support Vector Machine) ---\")\n",
    "    lsvc = LinearSVC(C=1000, penalty='l1', max_iter=3000, dual=False)\n",
    "    lsvc.fit(tfidfv, y_train)\n",
    "    predicted = lsvc.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "    print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "    print(\"F1 Score:\", f1_score(y_test, predicted, average='macro'))\n",
    "\n",
    "    print(\"--- (5) 결정 트리(Decision Tree) ---\")\n",
    "    tree = DecisionTreeClassifier(max_depth=10, random_state=0)\n",
    "    tree.fit(tfidfv, y_train)\n",
    "    predicted = tree.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "    print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값\n",
    "    print(\"F1 Score:\", f1_score(y_test, predicted, average='macro'))\n",
    "\n",
    "    print(\"--- (6) 랜덤 포레스트(Random Forest) ---\")\n",
    "    forest = RandomForestClassifier(n_estimators=5, random_state=0)\n",
    "    forest.fit(tfidfv, y_train)\n",
    "    predicted = forest.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "    print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "    print(\"F1 Score:\", f1_score(y_test, predicted, average='macro'))\n",
    "\n",
    "    print(\"--- (7) 그래디언트 부스팅 트리(GradientBoostingClassifier) ---\")\n",
    "    grbt = GradientBoostingClassifier(random_state=0) # verbose=3\n",
    "    grbt.fit(tfidfv, y_train)\n",
    "    predicted = grbt.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "    print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "    print(\"F1 Score:\", f1_score(y_test, predicted, average='macro'))\n",
    "\n",
    "    print(\"--- (8) 보팅(Voting) ---\")\n",
    "    voting_classifier = VotingClassifier(estimators=[\n",
    "         ('lr', LogisticRegression(C=10000, penalty='l2')),\n",
    "        ('cb', ComplementNB()),\n",
    "        ('grbt', GradientBoostingClassifier(random_state=0))\n",
    "    ], voting='soft', n_jobs=-1)\n",
    "    voting_classifier.fit(tfidfv, y_train)\n",
    "    predicted = voting_classifier.predict(tfidfv_test) #테스트 데이터에 대한 예측\n",
    "    print(\"정확도:\", accuracy_score(y_test, predicted)) #예측값과 실제값 비교\n",
    "    print(\"F1 Score:\", f1_score(y_test, predicted, average='macro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f53c2d-782d-4fa2-a8a0-fafe9d7cd4e4",
   "metadata": {},
   "source": [
    "# 회고\n",
    "- 다양한 비교군을 두고 실험하는 것이 의미가 있고\n",
    "- 잘 모듈화 시켜 놓으면 다른 데이터에 대해서도 해당 모듈을 이용해서 보다 빠르게 성능 비교를 해볼 수 있을것 같음\n",
    "- 시간이 모자라서 실험을 다 끝내지 못함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e531878-b651-4899-b749-0db55ce3d7d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
