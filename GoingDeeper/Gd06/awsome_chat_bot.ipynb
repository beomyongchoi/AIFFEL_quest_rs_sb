{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nCqCU_ESSZt8"
   },
   "source": [
    "# Project: 멋진 챗봇 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u_RwmvP2TMEa",
    "outputId": "67b5c211-c3a7-4722-dae8-de367cfd46e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /usr/local/lib/python3.12/dist-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.26.4)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.3.0.post1)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open>=1.8.1->gensim) (1.17.3)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
      "/bin/bash: line 1: pin: command not found\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "!pip install nltk\n",
    "!pin install pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "StW2k1jETrZs",
    "outputId": "cc4c5f24-310a-4642-ee0e-1ad75fe6d016"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.4\n",
      "2.2.2\n",
      "2.8.0+cu126\n",
      "3.9.1\n",
      "4.3.3\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "import torch\n",
    "import nltk\n",
    "import gensim\n",
    "\n",
    "print(numpy.__version__)\n",
    "print(pandas.__version__)\n",
    "print(torch.__version__)\n",
    "print(nltk.__version__)\n",
    "print(gensim.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Kfrq87xSc-C"
   },
   "source": [
    "## Step 1. 데이터 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YJQTJ7isTyIC",
    "outputId": "f8456b5a-09d2-4e89-cf88-81f1fa61cf2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 질문 개수: 11823\n",
      "총 답변 개수: 11823\n",
      "\n",
      "첫 5개 질문:\n",
      "Q1: 12시 땡!\n",
      "Q2: 1지망 학교 떨어졌어\n",
      "Q3: 3박4일 놀러가고 싶다\n",
      "Q4: 3박4일 정도 놀러가고 싶다\n",
      "Q5: PPL 심하네\n",
      "\n",
      "첫 5개 답변:\n",
      "A1: 하루가 또 가네요.\n",
      "A2: 위로해 드립니다.\n",
      "A3: 여행은 언제나 좋죠.\n",
      "A4: 여행은 언제나 좋죠.\n",
      "A5: 눈살이 찌푸려지죠.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV 파일 읽기\n",
    "df = pd.read_csv('ChatbotData.csv')\n",
    "\n",
    "# 질문과 답변을 각각 변수에 저장\n",
    "questions = df['Q'].tolist()  # 또는 df['Q'].values\n",
    "answers = df['A'].tolist()    # 또는 df['A'].values\n",
    "\n",
    "# 확인해보기\n",
    "print(f\"총 질문 개수: {len(questions)}\")\n",
    "print(f\"총 답변 개수: {len(answers)}\")\n",
    "print(\"\\n첫 5개 질문:\")\n",
    "for i in range(5):\n",
    "    print(f\"Q{i+1}: {questions[i]}\")\n",
    "print(\"\\n첫 5개 답변:\")\n",
    "for i in range(5):\n",
    "    print(f\"A{i+1}: {answers[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJgSiQIuSi2x"
   },
   "source": [
    "## Step 2. 데이터 정제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6pvIgVtxT8L3",
    "outputId": "e3eb7f68-3afa-4ed4-ae6b-ee76f4cc8454"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 결과:\n",
      "원본: 안녕하세요! How are YOU?\n",
      "전처리: 안녕하세요! how are you?\n",
      "--------------------------------------------------\n",
      "원본: 오늘 날씨가 좋네요~~ ^^\n",
      "전처리: 오늘 날씨가 좋네요\n",
      "--------------------------------------------------\n",
      "원본: Hello123 @#$% 테스트입니다!\n",
      "전처리: hello123 테스트입니다!\n",
      "--------------------------------------------------\n",
      "원본:    여러   공백    처리   테스트   \n",
      "전처리: 여러 공백 처리 테스트\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_sentence(sentence):\n",
    "    \"\"\"\n",
    "    문장 전처리 함수\n",
    "    1. 영문자를 모두 소문자로 변환\n",
    "    2. 영문자, 한글, 숫자, 주요 특수문자만 남기고 나머지 제거\n",
    "    \"\"\"\n",
    "    # 1. 소문자로 변환\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    # 2. 영문자, 한글, 숫자, 주요 특수문자만 남기기\n",
    "    # 한글: ㄱ-ㅎ, ㅏ-ㅣ, 가-힣\n",
    "    # 영문자: a-z\n",
    "    # 숫자: 0-9\n",
    "    # 주요 특수문자: 공백, 마침표, 쉼표, 물음표, 느낌표, 하이픈 등\n",
    "    sentence = re.sub(r'[^a-z가-힣ㄱ-ㅎㅏ-ㅣ0-9\\s.,?!-]', '', sentence)\n",
    "\n",
    "    # 연속된 공백을 하나로 통합\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "\n",
    "    # 문장 앞뒤 공백 제거\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    return sentence\n",
    "\n",
    "# 테스트\n",
    "test_sentences = [\n",
    "    \"안녕하세요! How are YOU?\",\n",
    "    \"오늘 날씨가 좋네요~~ ^^\",\n",
    "    \"Hello123 @#$% 테스트입니다!\",\n",
    "    \"   여러   공백    처리   테스트   \"\n",
    "]\n",
    "\n",
    "print(\"전처리 결과:\")\n",
    "for sentence in test_sentences:\n",
    "    processed = preprocess_sentence(sentence)\n",
    "    print(f\"원본: {sentence}\")\n",
    "    print(f\"전처리: {processed}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L3LqVKT0T-pl",
    "outputId": "a13aed59-fdda-4b01-fe6f-393d081c647b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리 시작...\n",
      "전처리 완료!\n",
      "\n",
      "전처리된 질문 개수: 11823\n",
      "전처리된 답변 개수: 11823\n",
      "\n",
      "=== 전처리 전후 비교 (첫 5개) ===\n",
      "\n",
      "[1번째]\n",
      "원본 질문: 12시 땡!\n",
      "전처리 질문: 12시 땡!\n",
      "원본 답변: 하루가 또 가네요.\n",
      "전처리 답변: 하루가 또 가네요.\n",
      "------------------------------------------------------------\n",
      "\n",
      "[2번째]\n",
      "원본 질문: 1지망 학교 떨어졌어\n",
      "전처리 질문: 1지망 학교 떨어졌어\n",
      "원본 답변: 위로해 드립니다.\n",
      "전처리 답변: 위로해 드립니다.\n",
      "------------------------------------------------------------\n",
      "\n",
      "[3번째]\n",
      "원본 질문: 3박4일 놀러가고 싶다\n",
      "전처리 질문: 3박4일 놀러가고 싶다\n",
      "원본 답변: 여행은 언제나 좋죠.\n",
      "전처리 답변: 여행은 언제나 좋죠.\n",
      "------------------------------------------------------------\n",
      "\n",
      "[4번째]\n",
      "원본 질문: 3박4일 정도 놀러가고 싶다\n",
      "전처리 질문: 3박4일 정도 놀러가고 싶다\n",
      "원본 답변: 여행은 언제나 좋죠.\n",
      "전처리 답변: 여행은 언제나 좋죠.\n",
      "------------------------------------------------------------\n",
      "\n",
      "[5번째]\n",
      "원본 질문: PPL 심하네\n",
      "전처리 질문: ppl 심하네\n",
      "원본 답변: 눈살이 찌푸려지죠.\n",
      "전처리 답변: 눈살이 찌푸려지죠.\n",
      "------------------------------------------------------------\n",
      "\n",
      "빈 질문 개수: 0\n",
      "빈 답변 개수: 0\n"
     ]
    }
   ],
   "source": [
    "# 앞서 정의한 preprocess_sentence 함수 사용\n",
    "\n",
    "# 1. questions와 answers 리스트에 전처리 적용\n",
    "print(\"전처리 시작...\")\n",
    "\n",
    "# 질문 전처리\n",
    "processed_questions = []\n",
    "for question in questions:\n",
    "    processed_q = preprocess_sentence(question)\n",
    "    processed_questions.append(processed_q)\n",
    "\n",
    "# 답변 전처리\n",
    "processed_answers = []\n",
    "for answer in answers:\n",
    "    processed_a = preprocess_sentence(answer)\n",
    "    processed_answers.append(processed_a)\n",
    "\n",
    "print(\"전처리 완료!\")\n",
    "\n",
    "# 2. 전처리 결과 확인\n",
    "print(f\"\\n전처리된 질문 개수: {len(processed_questions)}\")\n",
    "print(f\"전처리된 답변 개수: {len(processed_answers)}\")\n",
    "\n",
    "print(\"\\n=== 전처리 전후 비교 (첫 5개) ===\")\n",
    "for i in range(min(5, len(questions))):\n",
    "    print(f\"\\n[{i+1}번째]\")\n",
    "    print(f\"원본 질문: {questions[i]}\")\n",
    "    print(f\"전처리 질문: {processed_questions[i]}\")\n",
    "    print(f\"원본 답변: {answers[i]}\")\n",
    "    print(f\"전처리 답변: {processed_answers[i]}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# 3. 빈 문장이나 너무 짧은 문장 확인\n",
    "empty_questions = [i for i, q in enumerate(processed_questions) if len(q.strip()) == 0]\n",
    "empty_answers = [i for i, a in enumerate(processed_answers) if len(a.strip()) == 0]\n",
    "\n",
    "print(f\"\\n빈 질문 개수: {len(empty_questions)}\")\n",
    "print(f\"빈 답변 개수: {len(empty_answers)}\")\n",
    "\n",
    "if empty_questions:\n",
    "    print(f\"빈 질문 인덱스: {empty_questions[:0]}\")  # 처음 10개만\n",
    "if empty_answers:\n",
    "    print(f\"빈 답변 인덱스: {empty_answers[:10]}\")    # 처음 10개만"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KfzGsk02SofB"
   },
   "source": [
    "### 긴 문장 제외 (20단어 초과 문장 없애기)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Ekzp6LQ2VjNM",
    "outputId": "577a4a08-e9c6-422c-84dc-66fcf0a6276a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 질문 길이 분포 ===\n",
      "총 문장 수: 11823\n",
      "평균 길이: 3.59\n",
      "중간값: 3.00\n",
      "최대 길이: 15\n",
      "최소 길이: 1\n",
      "표준편차: 1.62\n",
      "분위수:\n",
      "  25%: 2.0\n",
      "  50%: 3.0\n",
      "  75%: 4.0\n",
      "  90%: 6.0\n",
      "  95%: 7.0\n",
      "  99%: 8.0\n",
      "\n",
      "=== 답변 길이 분포 ===\n",
      "총 문장 수: 11823\n",
      "평균 길이: 3.69\n",
      "중간값: 3.00\n",
      "최대 길이: 21\n",
      "최소 길이: 1\n",
      "표준편차: 1.86\n",
      "분위수:\n",
      "  25%: 2.0\n",
      "  50%: 3.0\n",
      "  75%: 5.0\n",
      "  90%: 6.0\n",
      "  95%: 7.0\n",
      "  99%: 10.0\n",
      "\n",
      "=== 질문 길이별 빈도 (상위 30개) ===\n",
      "길이  1:  625개 (  5.3%)\n",
      "길이  2: 2498개 ( 21.1%)\n",
      "길이  3: 3342개 ( 28.3%)\n",
      "길이  4: 2512개 ( 21.2%)\n",
      "길이  5: 1455개 ( 12.3%)\n",
      "길이  6:  754개 (  6.4%)\n",
      "길이  7:  361개 (  3.1%)\n",
      "길이  8:  163개 (  1.4%)\n",
      "길이  9:   70개 (  0.6%)\n",
      "길이 10:   25개 (  0.2%)\n",
      "길이 11:   10개 (  0.1%)\n",
      "길이 12:    7개 (  0.1%)\n",
      "길이 15:    1개 (  0.0%)\n",
      "\n",
      "=== 답변 길이별 빈도 (상위 30개) ===\n",
      "길이  1:  739개 (  6.3%)\n",
      "길이  2: 2472개 ( 20.9%)\n",
      "길이  3: 3117개 ( 26.4%)\n",
      "길이  4: 2384개 ( 20.2%)\n",
      "길이  5: 1436개 ( 12.1%)\n",
      "길이  6:  814개 (  6.9%)\n",
      "길이  7:  415개 (  3.5%)\n",
      "길이  8:  213개 (  1.8%)\n",
      "길이  9:   92개 (  0.8%)\n",
      "길이 10:   64개 (  0.5%)\n",
      "길이 11:   30개 (  0.3%)\n",
      "길이 12:   23개 (  0.2%)\n",
      "길이 13:    8개 (  0.1%)\n",
      "길이 14:    6개 (  0.1%)\n",
      "길이 15:    4개 (  0.0%)\n",
      "길이 16:    4개 (  0.0%)\n",
      "길이 20:    1개 (  0.0%)\n",
      "길이 21:    1개 (  0.0%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-1957013160.py:74: UserWarning: Glyph 44600 (\\N{HANGUL SYLLABLE GIL}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipython-input-1957013160.py:74: UserWarning: Glyph 51060 (\\N{HANGUL SYLLABLE I}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipython-input-1957013160.py:74: UserWarning: Glyph 45800 (\\N{HANGUL SYLLABLE DAN}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipython-input-1957013160.py:74: UserWarning: Glyph 50612 (\\N{HANGUL SYLLABLE EO}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipython-input-1957013160.py:74: UserWarning: Glyph 49688 (\\N{HANGUL SYLLABLE SU}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipython-input-1957013160.py:74: UserWarning: Glyph 48712 (\\N{HANGUL SYLLABLE BIN}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipython-input-1957013160.py:74: UserWarning: Glyph 46020 (\\N{HANGUL SYLLABLE DO}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipython-input-1957013160.py:74: UserWarning: Glyph 51656 (\\N{HANGUL SYLLABLE JIL}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipython-input-1957013160.py:74: UserWarning: Glyph 47928 (\\N{HANGUL SYLLABLE MUN}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipython-input-1957013160.py:74: UserWarning: Glyph 48516 (\\N{HANGUL SYLLABLE BUN}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipython-input-1957013160.py:74: UserWarning: Glyph 54252 (\\N{HANGUL SYLLABLE PO}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipython-input-1957013160.py:74: UserWarning: Glyph 45813 (\\N{HANGUL SYLLABLE DAB}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipython-input-1957013160.py:74: UserWarning: Glyph 48320 (\\N{HANGUL SYLLABLE BYEON}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipython-input-1957013160.py:74: UserWarning: Glyph 48708 (\\N{HANGUL SYLLABLE BI}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/tmp/ipython-input-1957013160.py:74: UserWarning: Glyph 44368 (\\N{HANGUL SYLLABLE GYO}) missing from font(s) DejaVu Sans.\n",
      "  plt.tight_layout()\n",
      "/usr/local/lib/python3.12/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 48712 (\\N{HANGUL SYLLABLE BIN}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.12/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 46020 (\\N{HANGUL SYLLABLE DO}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.12/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 51656 (\\N{HANGUL SYLLABLE JIL}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.12/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 47928 (\\N{HANGUL SYLLABLE MUN}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.12/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 44600 (\\N{HANGUL SYLLABLE GIL}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.12/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 51060 (\\N{HANGUL SYLLABLE I}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.12/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 48516 (\\N{HANGUL SYLLABLE BUN}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.12/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 54252 (\\N{HANGUL SYLLABLE PO}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.12/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 45800 (\\N{HANGUL SYLLABLE DAN}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.12/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 50612 (\\N{HANGUL SYLLABLE EO}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.12/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 49688 (\\N{HANGUL SYLLABLE SU}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.12/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 45813 (\\N{HANGUL SYLLABLE DAB}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.12/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 48320 (\\N{HANGUL SYLLABLE BYEON}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.12/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 48708 (\\N{HANGUL SYLLABLE BI}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n",
      "/usr/local/lib/python3.12/dist-packages/IPython/core/pylabtools.py:151: UserWarning: Glyph 44368 (\\N{HANGUL SYLLABLE GYO}) missing from font(s) DejaVu Sans.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1500x500 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 질문 길이 범위별 분포 ===\n",
      "1-5 단어    : 10432개 ( 88.2%)\n",
      "6-10 단어   : 1373개 ( 11.6%)\n",
      "11-15 단어  :   18개 (  0.2%)\n",
      "16-20 단어  :    0개 (  0.0%)\n",
      "21-30 단어  :    0개 (  0.0%)\n",
      "31+ 단어    :    0개 (  0.0%)\n",
      "\n",
      "=== 답변 길이 범위별 분포 ===\n",
      "1-5 단어    : 10148개 ( 85.8%)\n",
      "6-10 단어   : 1598개 ( 13.5%)\n",
      "11-15 단어  :   71개 (  0.6%)\n",
      "16-20 단어  :    5개 (  0.0%)\n",
      "21-30 단어  :    1개 (  0.0%)\n",
      "31+ 단어    :    0개 (  0.0%)\n",
      "\n",
      "=== 특이사항 체크 ===\n",
      "빈 질문: 0개\n",
      "빈 답변: 0개\n",
      "20단어 초과 질문: 0개\n",
      "20단어 초과 답변: 1개\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. 기본 길이 분포 계산\n",
    "def analyze_length_distribution(sentences, name):\n",
    "    \"\"\"문장들의 길이 분포를 분석하는 함수\"\"\"\n",
    "    lengths = [len(sentence.split()) for sentence in sentences]  # 공백 기준 단어 수\n",
    "\n",
    "    print(f\"\\n=== {name} 길이 분포 ===\")\n",
    "    print(f\"총 문장 수: {len(sentences)}\")\n",
    "    print(f\"평균 길이: {np.mean(lengths):.2f}\")\n",
    "    print(f\"중간값: {np.median(lengths):.2f}\")\n",
    "    print(f\"최대 길이: {max(lengths)}\")\n",
    "    print(f\"최소 길이: {min(lengths)}\")\n",
    "    print(f\"표준편차: {np.std(lengths):.2f}\")\n",
    "\n",
    "    # 분위수 정보\n",
    "    percentiles = [25, 50, 75, 90, 95, 99]\n",
    "    print(\"분위수:\")\n",
    "    for p in percentiles:\n",
    "        print(f\"  {p}%: {np.percentile(lengths, p):.1f}\")\n",
    "\n",
    "    return lengths\n",
    "\n",
    "# 질문과 답변 길이 분포 분석\n",
    "question_lengths = analyze_length_distribution(processed_questions, \"질문\")\n",
    "answer_lengths = analyze_length_distribution(processed_answers, \"답변\")\n",
    "\n",
    "# 2. 길이별 빈도 분포\n",
    "def show_frequency_distribution(lengths, name, max_show=30):\n",
    "    \"\"\"길이별 빈도 분포를 보여주는 함수\"\"\"\n",
    "    print(f\"\\n=== {name} 길이별 빈도 (상위 {max_show}개) ===\")\n",
    "\n",
    "    from collections import Counter\n",
    "    length_counts = Counter(lengths)\n",
    "\n",
    "    for length in sorted(length_counts.keys())[:max_show]:\n",
    "        count = length_counts[length]\n",
    "        percentage = (count / len(lengths)) * 100\n",
    "        print(f\"길이 {length:2d}: {count:4d}개 ({percentage:5.1f}%)\")\n",
    "\n",
    "show_frequency_distribution(question_lengths, \"질문\")\n",
    "show_frequency_distribution(answer_lengths, \"답변\")\n",
    "\n",
    "# 3. 시각화 (matplotlib 사용)\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# 질문 길이 히스토그램\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(question_lengths, bins=30, alpha=0.7, color='blue', edgecolor='black')\n",
    "plt.title('질문 길이 분포')\n",
    "plt.xlabel('길이 (단어 수)')\n",
    "plt.ylabel('빈도')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 답변 길이 히스토그램\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(answer_lengths, bins=30, alpha=0.7, color='red', edgecolor='black')\n",
    "plt.title('답변 길이 분포')\n",
    "plt.xlabel('길이 (단어 수)')\n",
    "plt.ylabel('빈도')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 질문 vs 답변 길이 비교\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(question_lengths, bins=20, alpha=0.5, label='질문', color='blue')\n",
    "plt.hist(answer_lengths, bins=20, alpha=0.5, label='답변', color='red')\n",
    "plt.title('질문 vs 답변 길이 비교')\n",
    "plt.xlabel('길이 (단어 수)')\n",
    "plt.ylabel('빈도')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 4. 특정 길이 범위의 문장 개수\n",
    "def count_by_length_range(lengths, name):\n",
    "    \"\"\"길이 범위별 문장 개수를 계산\"\"\"\n",
    "    print(f\"\\n=== {name} 길이 범위별 분포 ===\")\n",
    "    ranges = [(1, 5), (6, 10), (11, 15), (16, 20), (21, 30), (31, float('inf'))]\n",
    "\n",
    "    for start, end in ranges:\n",
    "        if end == float('inf'):\n",
    "            count = sum(1 for l in lengths if l >= start)\n",
    "            range_str = f\"{start}+ 단어\"\n",
    "        else:\n",
    "            count = sum(1 for l in lengths if start <= l <= end)\n",
    "            range_str = f\"{start}-{end} 단어\"\n",
    "\n",
    "        percentage = (count / len(lengths)) * 100\n",
    "        print(f\"{range_str:10s}: {count:4d}개 ({percentage:5.1f}%)\")\n",
    "\n",
    "count_by_length_range(question_lengths, \"질문\")\n",
    "count_by_length_range(answer_lengths, \"답변\")\n",
    "\n",
    "# 5. 빈 문장이나 너무 긴 문장 체크\n",
    "print(f\"\\n=== 특이사항 체크 ===\")\n",
    "empty_questions = sum(1 for s in processed_questions if len(s.strip()) == 0)\n",
    "empty_answers = sum(1 for s in processed_answers if len(s.strip()) == 0)\n",
    "print(f\"빈 질문: {empty_questions}개\")\n",
    "print(f\"빈 답변: {empty_answers}개\")\n",
    "\n",
    "long_questions = sum(1 for l in question_lengths if l > 20)\n",
    "long_answers = sum(1 for l in answer_lengths if l > 20)\n",
    "print(f\"20단어 초과 질문: {long_questions}개\")\n",
    "print(f\"20단어 초과 답변: {long_answers}개\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p8IyIovpS0ly"
   },
   "source": [
    "## Step 3. 데이터 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "gsL8pK-OUGJH"
   },
   "outputs": [],
   "source": [
    "# !pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "ogSKbQ12VAY7"
   },
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZzpGM8ghftqQ",
    "outputId": "7218117a-7766-42df-cab8-5b5ea68f1087"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentencePiece 모델 훈련 중...\n",
      "SentencePiece 모델 로드 완료\n"
     ]
    }
   ],
   "source": [
    "# 수정된 셀 9: SentencePiece 모델을 먼저 생성하고 로드한 후 사용\n",
    "\n",
    "def train_sentencepiece_model(sentences, model_name='chatbot_sp', vocab_size=5000):\n",
    "    \"\"\"SentencePiece 모델 훈련 함수\"\"\"\n",
    "    with open('temp_corpus.txt', 'w', encoding='utf-8') as f:\n",
    "        for sentence in sentences:\n",
    "            f.write(sentence + '\\n')\n",
    "\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input='temp_corpus.txt',\n",
    "        model_prefix=model_name,\n",
    "        vocab_size=vocab_size,\n",
    "        character_coverage=0.9995,\n",
    "        model_type='bpe',\n",
    "        unk_piece='<unk>',\n",
    "        bos_piece='<start>',\n",
    "        eos_piece='<end>',\n",
    "        pad_piece='<pad>'\n",
    "    )\n",
    "    return f'{model_name}.model'\n",
    "\n",
    "# 1. 먼저 SentencePiece 모델 훈련 및 로드\n",
    "print(\"SentencePiece 모델 훈련 중...\")\n",
    "all_text = processed_questions + processed_answers\n",
    "model_path = train_sentencepiece_model(all_text)\n",
    "\n",
    "# 2. SentencePiece 프로세서 초기화\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(model_path)\n",
    "print(\"SentencePiece 모델 로드 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "iuRilnf3hz3h"
   },
   "outputs": [],
   "source": [
    "def sp_tokenize(sentence):\n",
    "    \"\"\"SentencePiece를 사용한 토큰화 함수\"\"\"\n",
    "    return sp.encode_as_pieces(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AV3rHAZAUIUU",
    "outputId": "d9617fb1-ca48-44a8-895e-94f7e2dfdc28"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "코퍼스 구축 시작...\n",
      "원본 데이터 개수: 11823\n",
      "필터링 후 데이터 개수: 7717\n",
      "제거된 데이터 개수: 4106\n",
      "\n",
      "=== 코퍼스 구축 결과 ===\n",
      "질문 코퍼스 크기: 7717\n",
      "답변 코퍼스 크기: 7717\n",
      "\n",
      "=== 첫 5개 예시 ===\n",
      "\n",
      "[1]\n",
      "질문 토큰: ['▁1', '2', '시', '▁땡', '!']\n",
      "답변 토큰: ['▁하루', '가', '▁또', '▁가', '네요', '.']\n",
      "질문 복원: 12시 땡!\n",
      "답변 복원: 하루가 또 가네요.\n",
      "\n",
      "[2]\n",
      "질문 토큰: ['▁1', '지', '망', '▁학교', '▁떨어졌어']\n",
      "답변 토큰: ['▁위로해', '▁드', '립니다', '.']\n",
      "질문 복원: 1지망 학교 떨어졌어\n",
      "답변 복원: 위로해 드립니다.\n",
      "\n",
      "[3]\n",
      "질문 토큰: ['▁3', '박', '4', '일', '▁놀러가고', '▁싶다']\n",
      "답변 토큰: ['▁여행', '은', '▁언제나', '▁좋죠', '.']\n",
      "질문 복원: 3박4일 놀러가고 싶다\n",
      "답변 복원: 여행은 언제나 좋죠.\n",
      "\n",
      "[4]\n",
      "질문 토큰: ['▁', 'p', 'p', 'l', '▁심', '하네']\n",
      "답변 토큰: ['▁눈', '살', '이', '▁찌', '푸', '려', '지', '죠', '.']\n",
      "질문 복원: ppl 심하네\n",
      "답변 복원: 눈살이 찌푸려지죠.\n",
      "\n",
      "[5]\n",
      "질문 토큰: ['▁s', 'd', '카', '드', '▁망', '가', '졌어']\n",
      "답변 토큰: ['▁다시', '▁새로', '▁사는', '▁게', '▁마음', '▁편', '해요', '.']\n",
      "질문 복원: sd카드 망가졌어\n",
      "답변 복원: 다시 새로 사는 게 마음 편해요.\n",
      "\n",
      "=== 토큰 길이 통계 ===\n",
      "질문 길이 - 평균: 6.20, 최대: 20, 최소: 1\n",
      "답변 길이 - 평균: 6.67, 최대: 20, 최소: 1\n"
     ]
    }
   ],
   "source": [
    "def build_corpus(source_sentences, target_sentences, tokenizer, max_length=20):\n",
    "    \"\"\"\n",
    "    코퍼스를 구축하는 함수\n",
    "\n",
    "    Args:\n",
    "        source_sentences: 소스 문장 리스트 (질문)\n",
    "        target_sentences: 타겟 문장 리스트 (답변)\n",
    "        tokenizer: 토크나이저 함수 (sentencepiece)\n",
    "        max_length: 최대 토큰 길이 (이보다 긴 문장 제외)\n",
    "\n",
    "    Returns:\n",
    "        tuple: (processed_sources, processed_targets) - 토큰화된 문장들\n",
    "    \"\"\"\n",
    "\n",
    "    processed_sources = []\n",
    "    processed_targets = []\n",
    "\n",
    "    # 중복 체크를 위한 set\n",
    "    seen_sources = set()\n",
    "    seen_targets = set()\n",
    "\n",
    "    print(f\"원본 데이터 개수: {len(source_sentences)}\")\n",
    "\n",
    "    for i, (source, target) in enumerate(zip(source_sentences, target_sentences)):\n",
    "        # 1. 전처리\n",
    "        processed_source = preprocess_sentence(source)\n",
    "        processed_target = preprocess_sentence(target)\n",
    "\n",
    "        # 빈 문장 제거\n",
    "        if not processed_source.strip() or not processed_target.strip():\n",
    "            continue\n",
    "\n",
    "        # 2. 토큰화\n",
    "        source_tokens = tokenizer(processed_source)\n",
    "        target_tokens = tokenizer(processed_target)\n",
    "\n",
    "        # 3. 길이 체크 (최대 길이 이상인 문장 제외)\n",
    "        if len(source_tokens) > max_length or len(target_tokens) > max_length:\n",
    "            continue\n",
    "\n",
    "        # 4. 중복 체크 (소스와 타겟을 각각 체크)\n",
    "        source_str = ' '.join(source_tokens)\n",
    "        target_str = ' '.join(target_tokens)\n",
    "\n",
    "        # 소스나 타겟 중 하나라도 중복이면 해당 쌍 전체를 제외\n",
    "        if source_str in seen_sources or target_str in seen_targets:\n",
    "            continue\n",
    "\n",
    "        # 중복이 아니면 추가\n",
    "        seen_sources.add(source_str)\n",
    "        seen_targets.add(target_str)\n",
    "        processed_sources.append(source_tokens)\n",
    "        processed_targets.append(target_tokens)\n",
    "\n",
    "    print(f\"필터링 후 데이터 개수: {len(processed_sources)}\")\n",
    "    print(f\"제거된 데이터 개수: {len(source_sentences) - len(processed_sources)}\")\n",
    "\n",
    "    return processed_sources, processed_targets\n",
    "\n",
    "# 4. 함수 실행\n",
    "print(\"코퍼스 구축 시작...\")\n",
    "que_corpus, ans_corpus = build_corpus(\n",
    "    source_sentences=processed_questions,  # 또는 questions (전처리된 것 사용)\n",
    "    target_sentences=processed_answers,    # 또는 answers (전처리된 것 사용)\n",
    "    tokenizer=sp_tokenize,\n",
    "    max_length=20  # 최대 토큰 길이 설정\n",
    ")\n",
    "\n",
    "# 결과 확인\n",
    "print(f\"\\n=== 코퍼스 구축 결과 ===\")\n",
    "print(f\"질문 코퍼스 크기: {len(que_corpus)}\")\n",
    "print(f\"답변 코퍼스 크기: {len(ans_corpus)}\")\n",
    "\n",
    "print(f\"\\n=== 첫 5개 예시 ===\")\n",
    "for i in range(min(5, len(que_corpus))):\n",
    "    print(f\"\\n[{i+1}]\")\n",
    "    print(f\"질문 토큰: {que_corpus[i]}\")\n",
    "    print(f\"답변 토큰: {ans_corpus[i]}\")\n",
    "    print(f\"질문 복원: {sp.decode_pieces(que_corpus[i])}\")\n",
    "    print(f\"답변 복원: {sp.decode_pieces(ans_corpus[i])}\")\n",
    "\n",
    "# 토큰 길이 분포 확인\n",
    "import numpy as np\n",
    "que_lengths = [len(tokens) for tokens in que_corpus]\n",
    "ans_lengths = [len(tokens) for tokens in ans_corpus]\n",
    "\n",
    "print(f\"\\n=== 토큰 길이 통계 ===\")\n",
    "print(f\"질문 길이 - 평균: {np.mean(que_lengths):.2f}, 최대: {max(que_lengths)}, 최소: {min(que_lengths)}\")\n",
    "print(f\"답변 길이 - 평균: {np.mean(ans_lengths):.2f}, 최대: {max(ans_lengths)}, 최소: {min(ans_lengths)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-IqXgckyS4_C"
   },
   "source": [
    "## Step 4. Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Q95U2QN5YkbF"
   },
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "kv = KeyedVectors.load_word2vec_format(\"ko.vec\", binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wEySOPxqZzma",
    "outputId": "b2bd6a52-c0fa-4d2b-fefd-5b8004b421e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 데이터 3배 증강 시작 ===\n",
      "원본 데이터 크기: 7717\n",
      "\n",
      "1단계: 질문 증강\n",
      "문장 증강 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7717/7717 [00:16<00:00, 461.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2단계: 답변 증강\n",
      "문장 증강 중...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7717/7717 [00:14<00:00, 521.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3단계: 3배 데이터셋 구성\n",
      "\n",
      "=== 증강 완료 ===\n",
      "원본 데이터: 7717쌍\n",
      "최종 데이터: 23151쌍\n",
      "증강 비율: 3.0배\n",
      "\n",
      "4단계: 토큰화\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 23151/23151 [00:00<00:00, 72629.85it/s]\n",
      "100%|██████████| 23151/23151 [00:00<00:00, 72002.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 최종 결과 ===\n",
      "최종 질문 코퍼스: 23151\n",
      "최종 답변 코퍼스: 23151\n",
      "\n",
      "=== 샘플 확인 (첫 3개 세트) ===\n",
      "\n",
      "--- 1번째 샘플 ---\n",
      "[원본] Q: ▁1 2 시 ▁땡 !\n",
      "[원본] A: ▁하루 가 ▁또 ▁가 네요 .\n",
      "[증강Q] Q: ▁1 2 시가 ▁땡 !\n",
      "[원본A] A: ▁하루 가 ▁또 ▁가 네요 .\n",
      "[원본Q] Q: ▁1 2 시 ▁땡 !\n",
      "[증강A] A: ▁하루 로 ▁또 ▁가 네요 .\n",
      "\n",
      "--- 2번째 샘플 ---\n",
      "[원본] Q: ▁1 지 망 ▁학교 ▁떨어졌어\n",
      "[원본] A: ▁위로해 ▁드 립니다 .\n",
      "[증강Q] Q: ▁1 지 오트 ▁학교 ▁떨어졌어\n",
      "[원본A] A: ▁위로해 ▁드 립니다 .\n",
      "[원본Q] Q: ▁1 지 망 ▁학교 ▁떨어졌어\n",
      "[증강A] A: ▁위로해 ▁드 립니다 희영\n",
      "\n",
      "--- 3번째 샘플 ---\n",
      "[원본] Q: ▁3 박 4 일 ▁놀러가고 ▁싶다\n",
      "[원본] A: ▁여행 은 ▁언제나 ▁좋죠 .\n",
      "[증강Q] Q: ▁3 박이 4 일 ▁놀러가고 ▁싶다\n",
      "[원본A] A: ▁여행 은 ▁언제나 ▁좋죠 .\n",
      "[원본Q] Q: ▁3 박 4 일 ▁놀러가고 ▁싶다\n",
      "[증강A] A: ▁여행 은 ▁언제나 ▁좋죠 희영\n",
      "\n",
      "que_corpus와 ans_corpus가 23151개로 업데이트되었습니다!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "def lexical_sub(sentence, wv):\n",
    "    \"\"\"개선된 lexical substitution 함수\"\"\"\n",
    "    tokens = sentence.split()\n",
    "    valid_tokens = [tok for tok in tokens if tok in wv]\n",
    "\n",
    "    if not valid_tokens:\n",
    "        return sentence\n",
    "\n",
    "    selected_tok = random.choice(valid_tokens)\n",
    "\n",
    "    try:\n",
    "        similar_words = wv.most_similar(selected_tok, topn=5)\n",
    "        similar_word = random.choice(similar_words)[0]\n",
    "        new_sentence = \" \".join([similar_word if tok == selected_tok else tok for tok in tokens])\n",
    "        return new_sentence\n",
    "    except:\n",
    "        return sentence\n",
    "\n",
    "def augment_sentences(sentences, wv):\n",
    "    \"\"\"문장 리스트를 증강하는 함수\"\"\"\n",
    "    augmented = []\n",
    "    print(\"문장 증강 중...\")\n",
    "\n",
    "    for sentence in tqdm(sentences):\n",
    "        if isinstance(sentence, list):  # 토큰화된 문장인 경우\n",
    "            sentence_str = ' '.join(sentence)\n",
    "            aug_sentence_str = lexical_sub(sentence_str, wv)\n",
    "            aug_sentence = aug_sentence_str.split()\n",
    "        else:  # 문자열인 경우\n",
    "            aug_sentence = lexical_sub(sentence, wv)\n",
    "\n",
    "        augmented.append(aug_sentence)\n",
    "\n",
    "    return augmented\n",
    "\n",
    "# 데이터 3배 증강 실행\n",
    "print(\"=== 데이터 3배 증강 시작 ===\")\n",
    "\n",
    "# 원본 데이터 (토큰화된 상태라면 문자열로 변환)\n",
    "if isinstance(que_corpus[0], list):\n",
    "    que_strings = [' '.join(tokens) for tokens in que_corpus]\n",
    "    ans_strings = [' '.join(tokens) for tokens in ans_corpus]\n",
    "else:\n",
    "    que_strings = que_corpus.copy()\n",
    "    ans_strings = ans_corpus.copy()\n",
    "\n",
    "print(f\"원본 데이터 크기: {len(que_strings)}\")\n",
    "\n",
    "# 1. 질문 증강\n",
    "print(\"\\n1단계: 질문 증강\")\n",
    "augmented_questions = augment_sentences(que_strings, kv)\n",
    "\n",
    "# 2. 답변 증강\n",
    "print(\"\\n2단계: 답변 증강\")\n",
    "augmented_answers = augment_sentences(ans_strings, kv)\n",
    "\n",
    "# 3. 3배 확장된 데이터셋 생성\n",
    "print(\"\\n3단계: 3배 데이터셋 구성\")\n",
    "\n",
    "# 최종 데이터 구성\n",
    "final_questions = []\n",
    "final_answers = []\n",
    "\n",
    "# 원본 데이터\n",
    "final_questions.extend(que_strings)\n",
    "final_answers.extend(ans_strings)\n",
    "\n",
    "# 증강된 질문 + 원본 답변\n",
    "final_questions.extend(augmented_questions)\n",
    "final_answers.extend(ans_strings)  # 원본 답변 재사용\n",
    "\n",
    "# 원본 질문 + 증강된 답변\n",
    "final_questions.extend(que_strings)  # 원본 질문 재사용\n",
    "final_answers.extend(augmented_answers)\n",
    "\n",
    "print(f\"\\n=== 증강 완료 ===\")\n",
    "print(f\"원본 데이터: {len(que_strings)}쌍\")\n",
    "print(f\"최종 데이터: {len(final_questions)}쌍\")\n",
    "print(f\"증강 비율: {len(final_questions) / len(que_strings):.1f}배\")\n",
    "\n",
    "# 4. 토큰화 (필요한 경우)\n",
    "print(\"\\n4단계: 토큰화\")\n",
    "if hasattr(sp, 'encode_as_pieces'):  # SentencePiece 토크나이저가 있는 경우\n",
    "    final_que_corpus = [sp.encode_as_pieces(q) for q in tqdm(final_questions)]\n",
    "    final_ans_corpus = [sp.encode_as_pieces(a) for a in tqdm(final_answers)]\n",
    "else:  # 간단한 공백 토큰화\n",
    "    final_que_corpus = [q.split() for q in final_questions]\n",
    "    final_ans_corpus = [a.split() for a in final_answers]\n",
    "\n",
    "# 5. 결과 확인\n",
    "print(f\"\\n=== 최종 결과 ===\")\n",
    "print(f\"최종 질문 코퍼스: {len(final_que_corpus)}\")\n",
    "print(f\"최종 답변 코퍼스: {len(final_ans_corpus)}\")\n",
    "\n",
    "print(f\"\\n=== 샘플 확인 (첫 3개 세트) ===\")\n",
    "original_size = len(que_strings)\n",
    "\n",
    "for i in range(3):\n",
    "    print(f\"\\n--- {i+1}번째 샘플 ---\")\n",
    "    # 원본\n",
    "    print(f\"[원본] Q: {final_questions[i]}\")\n",
    "    print(f\"[원본] A: {final_answers[i]}\")\n",
    "\n",
    "    # 증강된 질문 + 원본 답변\n",
    "    print(f\"[증강Q] Q: {final_questions[original_size + i]}\")\n",
    "    print(f\"[원본A] A: {final_answers[original_size + i]}\")\n",
    "\n",
    "    # 원본 질문 + 증강된 답변\n",
    "    print(f\"[원본Q] Q: {final_questions[2*original_size + i]}\")\n",
    "    print(f\"[증강A] A: {final_answers[2*original_size + i]}\")\n",
    "\n",
    "# 최종 변수 업데이트\n",
    "que_corpus = final_que_corpus\n",
    "ans_corpus = final_ans_corpus\n",
    "\n",
    "print(f\"\\nque_corpus와 ans_corpus가 {len(que_corpus)}개로 업데이트되었습니다!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AxyBdXJ-TAQR"
   },
   "source": [
    "## Step 5. 데이터 벡터화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tlj5Ponga3B8",
    "outputId": "6844a62c-80ac-43d3-8d3a-8fa2cb766ba2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 타겟 데이터에 특수 토큰 추가 ===\n",
      "원본 답변 예시: ['▁하루', '▁가', '▁또', '▁가', '▁', '네요', '▁.']\n",
      "토큰 추가 후: ['<start>', '▁하루', '▁가', '▁또', '▁가', '▁', '네요', '▁.', '<end>']\n",
      "처리된 답변 데이터 수: 23151\n",
      "\n",
      "=== 전체 단어 사전 구축 ===\n",
      "전체 토큰 수: 395168\n",
      "고유 토큰 수: 4401\n",
      "최종 어휘 크기: 4403\n",
      "특수 토큰: ['<pad>', '<unk>', '<start>', '<end>']\n",
      "가장 빈도 높은 토큰 10개: ['▁', '<start>', '<end>', '▁.', '▁이', '?', '▁다', '▁거예요', '▁나', '▁지']\n",
      "\n",
      "=== 시퀀스 길이 정보 ===\n",
      "질문 최대 길이: 27\n",
      "답변 최대 길이: 31\n",
      "\n",
      "=== 벡터화 진행 ===\n",
      "enc_train 형태: (23151, 27)\n",
      "dec_train 형태: (23151, 31)\n",
      "\n",
      "=== 벡터화 결과 확인 ===\n",
      "\n",
      "[1번째 예시]\n",
      "원본 질문: ['▁1', '▁2', '▁시', '▁땡', '▁', '!']\n",
      "벡터화된 질문: [ 285  286   84 2942    4  141    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "원본 답변: ['<start>', '▁하루', '▁가', '▁또', '▁가', '▁', '네요', '▁.', '<end>']\n",
      "벡터화된 답변: [  2 376  14  88  14   4  64   5   3   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
      "복원된 질문: ['▁1', '▁2', '▁시', '▁땡', '▁', '!']\n",
      "복원된 답변: ['<start>', '▁하루', '▁가', '▁또', '▁가', '▁', '네요', '▁.', '<end>']\n",
      "\n",
      "[2번째 예시]\n",
      "원본 질문: ['▁1', '▁지', '▁망', '▁학교', '▁떨어졌어']\n",
      "벡터화된 질문: [ 285   11  465  988 2346    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "원본 답변: ['<start>', '▁위로해', '▁드', '▁', '립니다', '▁.', '<end>']\n",
      "벡터화된 답변: [   2 2560  263    4 1833    5    3    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0]\n",
      "복원된 질문: ['▁1', '▁지', '▁망', '▁학교', '▁떨어졌어']\n",
      "복원된 답변: ['<start>', '▁위로해', '▁드', '▁', '립니다', '▁.', '<end>']\n",
      "\n",
      "[3번째 예시]\n",
      "원본 질문: ['▁3', '▁', '박', '▁4', '▁일', '▁놀러가고', '▁싶다']\n",
      "벡터화된 질문: [ 334    4  975  624   44 2943  154    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0]\n",
      "원본 답변: ['<start>', '▁여행', '▁은', '▁언제나', '▁좋죠', '▁.', '<end>']\n",
      "벡터화된 답변: [   2  421   37 1122  328    5    3    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0]\n",
      "복원된 질문: ['▁3', '▁', '박', '▁4', '▁일', '▁놀러가고', '▁싶다']\n",
      "복원된 답변: ['<start>', '▁여행', '▁은', '▁언제나', '▁좋죠', '▁.', '<end>']\n",
      "\n",
      "=== 최종 데이터 정보 ===\n",
      "어휘 크기: 4403\n",
      "훈련 데이터 수: 23151\n",
      "인코더 입력 크기: (23151, 27)\n",
      "디코더 입력 크기: (23151, 31)\n",
      "질문 최대 길이: 27\n",
      "답변 최대 길이: 31\n",
      "\n",
      "vocab_to_idx와 idx_to_vocab 딕셔너리가 생성되었습니다.\n",
      "이는 나중에 모델 예측 결과를 텍스트로 변환할 때 필요합니다.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "# 1. 타겟 데이터에 <start>와 <end> 토큰 추가\n",
    "print(\"=== 타겟 데이터에 특수 토큰 추가 ===\")\n",
    "\n",
    "# ans_corpus에 <start>와 <end> 토큰 추가\n",
    "ans_corpus_with_tokens = []\n",
    "for answer in ans_corpus:\n",
    "    # 각 답변에 시작과 종료 토큰 추가\n",
    "    answer_with_tokens = [\"<start>\"] + answer + [\"<end>\"]\n",
    "    ans_corpus_with_tokens.append(answer_with_tokens)\n",
    "\n",
    "print(f\"원본 답변 예시: {ans_corpus[0]}\")\n",
    "print(f\"토큰 추가 후: {ans_corpus_with_tokens[0]}\")\n",
    "print(f\"처리된 답변 데이터 수: {len(ans_corpus_with_tokens)}\")\n",
    "\n",
    "# 2. 전체 데이터에 대한 단어 사전 구축\n",
    "print(\"\\n=== 전체 단어 사전 구축 ===\")\n",
    "\n",
    "# 모든 토큰을 하나의 리스트로 수집\n",
    "all_tokens = []\n",
    "\n",
    "# 질문 데이터의 모든 토큰 추가\n",
    "for question in que_corpus:\n",
    "    all_tokens.extend(question)\n",
    "\n",
    "# 답변 데이터의 모든 토큰 추가 (특수 토큰 포함)\n",
    "for answer in ans_corpus_with_tokens:\n",
    "    all_tokens.extend(answer)\n",
    "\n",
    "# 토큰 빈도 계산\n",
    "token_counts = Counter(all_tokens)\n",
    "print(f\"전체 토큰 수: {len(all_tokens)}\")\n",
    "print(f\"고유 토큰 수: {len(token_counts)}\")\n",
    "\n",
    "# 빈도 기준으로 단어 사전 생성 (빈도 높은 순으로 정렬)\n",
    "vocab_size = len(token_counts)\n",
    "sorted_tokens = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# 특수 토큰들을 먼저 추가\n",
    "special_tokens = [\"<pad>\", \"<unk>\", \"<start>\", \"<end>\"]\n",
    "vocab_to_idx = {}\n",
    "idx_to_vocab = {}\n",
    "\n",
    "# 특수 토큰 인덱스 할당\n",
    "for i, token in enumerate(special_tokens):\n",
    "    vocab_to_idx[token] = i\n",
    "    idx_to_vocab[i] = token\n",
    "\n",
    "# 나머지 토큰들 인덱스 할당\n",
    "current_idx = len(special_tokens)\n",
    "for token, count in sorted_tokens:\n",
    "    if token not in vocab_to_idx:  # 이미 추가된 특수 토큰은 제외\n",
    "        vocab_to_idx[token] = current_idx\n",
    "        idx_to_vocab[current_idx] = token\n",
    "        current_idx += 1\n",
    "\n",
    "vocab_size = len(vocab_to_idx)\n",
    "print(f\"최종 어휘 크기: {vocab_size}\")\n",
    "print(f\"특수 토큰: {special_tokens}\")\n",
    "print(f\"가장 빈도 높은 토큰 10개: {[token for token, count in sorted_tokens[:10]]}\")\n",
    "\n",
    "# 3. 벡터화 함수 정의\n",
    "def tokenize_and_pad(sentences, vocab_to_idx, max_length=None):\n",
    "    \"\"\"\n",
    "    문장들을 인덱스로 변환하고 패딩 적용\n",
    "    \"\"\"\n",
    "    # 최대 길이 계산 (지정하지 않은 경우)\n",
    "    if max_length is None:\n",
    "        max_length = max(len(sentence) for sentence in sentences)\n",
    "\n",
    "    vectorized = []\n",
    "    for sentence in sentences:\n",
    "        # 토큰을 인덱스로 변환\n",
    "        indices = []\n",
    "        for token in sentence:\n",
    "            if token in vocab_to_idx:\n",
    "                indices.append(vocab_to_idx[token])\n",
    "            else:\n",
    "                indices.append(vocab_to_idx[\"<unk>\"])  # 없는 단어는 <unk>로\n",
    "\n",
    "        # 패딩 적용\n",
    "        if len(indices) < max_length:\n",
    "            # 부족한 부분은 <pad> 토큰으로 채움\n",
    "            indices.extend([vocab_to_idx[\"<pad>\"]] * (max_length - len(indices)))\n",
    "        else:\n",
    "            # 길이가 초과하면 자름\n",
    "            indices = indices[:max_length]\n",
    "\n",
    "        vectorized.append(indices)\n",
    "\n",
    "    return np.array(vectorized)\n",
    "\n",
    "# 4. 최대 길이 설정\n",
    "que_max_length = max(len(sentence) for sentence in que_corpus)\n",
    "ans_max_length = max(len(sentence) for sentence in ans_corpus_with_tokens)\n",
    "\n",
    "print(f\"\\n=== 시퀀스 길이 정보 ===\")\n",
    "print(f\"질문 최대 길이: {que_max_length}\")\n",
    "print(f\"답변 최대 길이: {ans_max_length}\")\n",
    "\n",
    "# 5. 벡터화 실행\n",
    "print(\"\\n=== 벡터화 진행 ===\")\n",
    "\n",
    "# 인코더 입력 데이터 (질문)\n",
    "enc_train = tokenize_and_pad(que_corpus, vocab_to_idx, max_length=que_max_length)\n",
    "\n",
    "# 디코더 입력 데이터 (답변 - 특수 토큰 포함)\n",
    "dec_train = tokenize_and_pad(ans_corpus_with_tokens, vocab_to_idx, max_length=ans_max_length)\n",
    "\n",
    "print(f\"enc_train 형태: {enc_train.shape}\")\n",
    "print(f\"dec_train 형태: {dec_train.shape}\")\n",
    "\n",
    "# 6. 결과 확인\n",
    "print(\"\\n=== 벡터화 결과 확인 ===\")\n",
    "for i in range(3):\n",
    "    print(f\"\\n[{i+1}번째 예시]\")\n",
    "    print(f\"원본 질문: {que_corpus[i]}\")\n",
    "    print(f\"벡터화된 질문: {enc_train[i]}\")\n",
    "    print(f\"원본 답변: {ans_corpus_with_tokens[i]}\")\n",
    "    print(f\"벡터화된 답변: {dec_train[i]}\")\n",
    "\n",
    "    # 벡터를 다시 토큰으로 변환해서 확인\n",
    "    restored_question = [idx_to_vocab[idx] for idx in enc_train[i] if idx != vocab_to_idx[\"<pad>\"]]\n",
    "    restored_answer = [idx_to_vocab[idx] for idx in dec_train[i] if idx != vocab_to_idx[\"<pad>\"]]\n",
    "    print(f\"복원된 질문: {restored_question}\")\n",
    "    print(f\"복원된 답변: {restored_answer}\")\n",
    "\n",
    "# 7. 데이터 정보 요약\n",
    "print(f\"\\n=== 최종 데이터 정보 ===\")\n",
    "print(f\"어휘 크기: {vocab_size}\")\n",
    "print(f\"훈련 데이터 수: {len(enc_train)}\")\n",
    "print(f\"인코더 입력 크기: {enc_train.shape}\")\n",
    "print(f\"디코더 입력 크기: {dec_train.shape}\")\n",
    "print(f\"질문 최대 길이: {que_max_length}\")\n",
    "print(f\"답변 최대 길이: {ans_max_length}\")\n",
    "\n",
    "# vocab_to_idx, idx_to_vocab 딕셔너리도 나중에 사용할 수 있도록 저장\n",
    "print(f\"\\nvocab_to_idx와 idx_to_vocab 딕셔너리가 생성되었습니다.\")\n",
    "print(f\"이는 나중에 모델 예측 결과를 텍스트로 변환할 때 필요합니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_m9E0lKigGx5"
   },
   "source": [
    "## Step 6. 훈련하기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QAjbN2n6TIIq"
   },
   "source": [
    "### 6-1. 모델 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1n1-x_3FiN1f",
    "outputId": "86e5bc9b-d8af-43a4-c33a-1eefaa60621f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Positional Encoding 구현\n",
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, (2*(i//2)) / np.float32(d_model))\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "\n",
    "    return sinusoid_table\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tS2kF9L-iX7a",
    "outputId": "438cfa84-e3b7-4a3a-9c42-875a96643d7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def generate_padding_mask(seq: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    seq: shape [batch_size, seq_len]의 입력 (토큰 ID 텐서)\n",
    "    반환: shape [batch_size, 1, 1, seq_len]의 패딩 마스크\n",
    "         (seq == 0)인 위치가 1, 나머지는 0\n",
    "    \"\"\"\n",
    "    # (seq == 0)은 불리언 텐서를 반환 -> float()로 형변환 -> (1.0 or 0.0)\n",
    "    # 차원 확장: [batch_size, seq_len] → [batch_size, 1, 1, seq_len]\n",
    "    return (seq == 0).unsqueeze(1).unsqueeze(2).float()\n",
    "\n",
    "\n",
    "def generate_lookahead_mask(size: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    size: 문장(시퀀스) 길이\n",
    "    반환: shape [size, size],\n",
    "         i < j (대각선 위)에 해당하는 위치가 1, 아닌 곳은 0\n",
    "         (미래 토큰을 가리기 위한 마스크)\n",
    "    \"\"\"\n",
    "    # triu(diagonal=1)은 주대각선 위가 1, 아래가 0인 텐서를 만들어 줌\n",
    "    return torch.triu(torch.ones(size, size), diagonal=1)\n",
    "\n",
    "\n",
    "def generate_masks(src: torch.Tensor, tgt: torch.Tensor):\n",
    "    \"\"\"\n",
    "    src, tgt: shape [batch_size, seq_len]\n",
    "    3가지 마스크를 반환:\n",
    "      - enc_mask: 인코더 입력용 패딩 마스크\n",
    "      - dec_enc_mask: 디코더-인코더 어텐션용 패딩 마스크\n",
    "      - dec_mask: 디코더 자기어텐션용 마스크(룩어헤드 + 패딩)\n",
    "\n",
    "    각각의 shape:\n",
    "      - enc_mask, dec_enc_mask: [batch_size, 1, 1, src_seq_len]\n",
    "      - dec_mask: [batch_size, 1, tgt_seq_len, tgt_seq_len]\n",
    "    \"\"\"\n",
    "    # 1) 인코더 입력용 패딩 마스크\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "    # 2) 디코더에서 인코더 값을 볼 때 사용하는 마스크 (src 마스크 재사용)\n",
    "    dec_enc_mask = generate_padding_mask(src)\n",
    "\n",
    "    # 3) 디코더 자기어텐션 마스크 (미래 토큰 방지 룩어헤드 + tgt 자체 패딩 마스크)\n",
    "    dec_lookahead_mask = generate_lookahead_mask(tgt.shape[1])  # [tgt_seq_len, tgt_seq_len]\n",
    "    dec_tgt_padding_mask = generate_padding_mask(tgt)           # [batch_size, 1, 1, tgt_seq_len]\n",
    "\n",
    "    # 룩어헤드 마스크를 (batch 차원과 head 차원을 가상으로) 확장\n",
    "    dec_lookahead_mask = dec_lookahead_mask.unsqueeze(0).unsqueeze(1)  # [1, 1, seq_len, seq_len]\n",
    "\n",
    "    # 패딩 + 룩어헤드 마스크 병합\n",
    "    # 브로드캐스팅에 의해 shape [batch_size, 1, tgt_seq_len, tgt_seq_len]이 됨\n",
    "\n",
    "    dec_tgt_padding_mask = dec_tgt_padding_mask.to(device)\n",
    "    dec_lookahead_mask = dec_lookahead_mask.to(device)\n",
    "\n",
    "    dec_mask = torch.max(dec_tgt_padding_mask, dec_lookahead_mask)\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "h_4cQzVCiZvM",
    "outputId": "1845f1cb-6e1d-4665-e48a-5c03065e2df2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # d_model을 num_heads로 나눈 만큼이 각 head가 담당할 차원 수\n",
    "        self.depth = d_model // num_heads\n",
    "\n",
    "        # Query, Key, Value를 구하는 선형 레이어\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # 최종적으로 head들의 출력을 결합해주는 선형 레이어\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Q, K, V:  [batch_size, num_heads, seq_len, depth]\n",
    "        mask:     [batch_size, 1, seq_len, seq_len] 혹은\n",
    "                  [batch_size, num_heads, seq_len, seq_len]\n",
    "                  (어텐션에서 제외할 위치=1, 사용할 위치=0)\n",
    "        \"\"\"\n",
    "        # d_k = depth\n",
    "        d_k = Q.size(-1)  # K.shape[-1]도 동일\n",
    "        # Q와 K의 전치 곱: (batch_size, num_heads, seq_len, seq_len)\n",
    "        QK = torch.matmul(Q, K.transpose(-1, -2))\n",
    "\n",
    "        # 스케일링\n",
    "        scaled_qk = QK / torch.sqrt(torch.tensor(d_k, dtype=torch.float32))\n",
    "\n",
    "        # 마스크가 있는 경우 -1e9(매우 작은 수)를 더하여 softmax 후 확률이 0에 가깝도록 처리\n",
    "        if mask is not None:\n",
    "            scaled_qk = scaled_qk + (mask * -1e9)\n",
    "\n",
    "        attentions = F.softmax(scaled_qk, dim=-1)  # (batch_size, num_heads, seq_len, seq_len)\n",
    "        out = torch.matmul(attentions, V)         # (batch_size, num_heads, seq_len, depth)\n",
    "\n",
    "        return out, attentions\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        \"\"\"\n",
    "        x: [batch_size, seq_len, d_model]\n",
    "        반환: [batch_size, num_heads, seq_len, depth]\n",
    "        \"\"\"\n",
    "        bsz, seq_len, _ = x.size()\n",
    "        # d_model -> (num_heads * depth)이므로 view로 재배치\n",
    "        x = x.view(bsz, seq_len, self.num_heads, self.depth)\n",
    "        # (batch_size, seq_len, num_heads, depth) -> (batch_size, num_heads, seq_len, depth)\n",
    "        x = x.permute(0, 2, 1, 3)\n",
    "        return x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"\n",
    "        x: [batch_size, num_heads, seq_len, depth]\n",
    "        반환: [batch_size, seq_len, d_model]\n",
    "        \"\"\"\n",
    "        bsz, num_heads, seq_len, depth = x.size()\n",
    "        # (batch_size, num_heads, seq_len, depth) -> (batch_size, seq_len, num_heads, depth)\n",
    "        x = x.permute(0, 2, 1, 3).contiguous()\n",
    "        x = x.view(bsz, seq_len, self.d_model)\n",
    "        return x\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Q, K, V: [batch_size, seq_len, d_model]\n",
    "        mask:    [batch_size, 1, seq_len, seq_len] 혹은\n",
    "                 [batch_size, num_heads, seq_len, seq_len]\n",
    "        \"\"\"\n",
    "        # W_q, W_k, W_v는 각각 (d_model -> d_model) 선형 변환\n",
    "        WQ = self.W_q(Q)  # [batch_size, seq_len, d_model]\n",
    "        WK = self.W_k(K)  # [batch_size, seq_len, d_model]\n",
    "        WV = self.W_v(V)  # [batch_size, seq_len, d_model]\n",
    "\n",
    "        # 멀티헤드 분할\n",
    "        WQ_splits = self.split_heads(WQ)  # [batch_size, num_heads, seq_len, depth]\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask\n",
    "        )\n",
    "\n",
    "        # head 결과 결합 후 최종 선형\n",
    "        out = self.combine_heads(out)  # [batch_size, seq_len, d_model]\n",
    "        out = self.linear(out)         # [batch_size, seq_len, d_model]\n",
    "\n",
    "        return out, attention_weights\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DPvT0nIYibo3",
    "outputId": "c746ed8c-f78b-46eb-eb2e-38069f7cd692"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.relu(self.fc1(x))  # 첫 번째 Dense + ReLU\n",
    "        out = self.fc2(out)          # 두 번째 Dense\n",
    "        return out\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QGrLj9Szidev",
    "outputId": "76e67c3d-7c1e-48d1-e2d4-980f5209c2b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        # nn.LayerNorm은 마지막 차원(d_model)을 기준으로 정규화\n",
    "        self.norm_1 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.norm_2 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "\n",
    "        self.do = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Multi-Head Attention 단계\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.do(out)\n",
    "        out = out + residual  # residual connection\n",
    "\n",
    "        # Position-Wise Feed Forward 단계\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out = out + residual  # residual connection\n",
    "\n",
    "        return out, enc_attn\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ub_cWjHPifOi",
    "outputId": "c02e14af-e33e-482c-d003-bd2b9ee0a6ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.norm_2 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "        self.norm_3 = nn.LayerNorm(d_model, eps=1e-6)\n",
    "\n",
    "        self.do = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        # Masked Multi-Head Attention\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, mask=padding_mask)\n",
    "        out = self.do(out)\n",
    "        out = out + residual\n",
    "\n",
    "        # Encoder-Decoder Multi-Head Attention (주의: Q, K, V 순서)\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out, dec_enc_attn = self.enc_dec_attn(out, enc_out, enc_out, mask=dec_enc_mask)\n",
    "        out = self.do(out)\n",
    "        out = out + residual\n",
    "\n",
    "        # Position-Wise Feed Forward Network\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.do(out)\n",
    "        out = out + residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Fn6i3ZVigyZ",
    "outputId": "f44bf9aa-9668-457c-df8b-8aa4f1759023"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)]\n",
    "        )\n",
    "        self.do = nn.Dropout(dropout)  # 필요 시 입력에 dropout 적용 가능\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        out = x\n",
    "        enc_attns = []\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        return out, enc_attns\n",
    "\n",
    "# 사용 예시: Encoder 인스턴스 생성 후 forward 호출\n",
    "# encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "# out, enc_attns = encoder(x, mask)\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kcA7KXB5iiHQ",
    "outputId": "3c5fd663-864a-47dd-c941-8b6fb4285328"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = nn.ModuleList(\n",
    "            [DecoderLayer(d_model, n_heads, d_ff, dropout) for _ in range(n_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x, enc_out, dec_enc_mask, padding_mask):\n",
    "        out = x\n",
    "        dec_attns = []\n",
    "        dec_enc_attns = []\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = self.dec_layers[i](out, enc_out, dec_enc_mask, padding_mask)\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "        return out, dec_attns, dec_enc_attns\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jg5gBw7bijmZ",
    "outputId": "f6add97f-7d29-4672-8566-abb0d8abf577"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, n_layers, d_model, n_heads, d_ff,\n",
    "                 src_vocab_size, tgt_vocab_size, pos_len,\n",
    "                 dropout=0.2, shared_fc=True, shared_emb=False):\n",
    "        super(Transformer, self).__init__()\n",
    "        # d_model은 스케일링에 사용되므로 float으로 저장\n",
    "        self.d_model = float(d_model)\n",
    "\n",
    "        # Embedding 레이어: shared_emb True면 동일한 임베딩을 사용합니다.\n",
    "        if shared_emb:\n",
    "            self.enc_emb = self.dec_emb = nn.Embedding(src_vocab_size, d_model)\n",
    "        else:\n",
    "            self.enc_emb = nn.Embedding(src_vocab_size, d_model)\n",
    "            self.dec_emb = nn.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        # Positional encoding (넘파이 버전 결과를 torch.Tensor로 변환)\n",
    "        pos_encoding_np = positional_encoding(pos_len, d_model)\n",
    "        # 파라미터로 등록하지 않고 고정값이므로 buffer로 등록합니다.\n",
    "        self.register_buffer(\"pos_encoding\", torch.tensor(pos_encoding_np, dtype=torch.float32))\n",
    "\n",
    "        self.do = nn.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "        self.shared_fc = shared_fc\n",
    "        if shared_fc:\n",
    "            # fc 레이어와 디코더 임베딩의 weight를 공유합니다.\n",
    "            self.fc.weight = self.dec_emb.weight\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        \"\"\"\n",
    "        emb: 임베딩 레이어\n",
    "        x: [batch_size, seq_len] (토큰 인덱스)\n",
    "        \"\"\"\n",
    "        seq_len = x.size(1)\n",
    "        out = emb(x)  # [batch_size, seq_len, d_model]\n",
    "        if self.shared_fc:\n",
    "            out = out * math.sqrt(self.d_model)\n",
    "        # pos_encoding: [pos_len, d_model] → [1, pos_len, d_model] 후 슬라이싱\n",
    "        out = out + self.pos_encoding[:seq_len, :].unsqueeze(0)\n",
    "        out = self.do(out)\n",
    "        return out\n",
    "\n",
    "    def forward(self, enc_in, dec_in, enc_mask, dec_enc_mask, dec_mask):\n",
    "        \"\"\"\n",
    "        enc_in: [batch_size, src_seq_len]\n",
    "        dec_in: [batch_size, tgt_seq_len]\n",
    "        enc_mask, dec_enc_mask, dec_mask: 마스킹 텐서들\n",
    "        \"\"\"\n",
    "        # Embedding 및 positional encoding 적용\n",
    "        enc_in_emb = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in_emb = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        # Encoder와 Decoder 통과\n",
    "        enc_out, enc_attns = self.encoder(enc_in_emb, enc_mask)\n",
    "        dec_out, dec_attns, dec_enc_attns = self.decoder(dec_in_emb, enc_out, dec_enc_mask, dec_mask)\n",
    "\n",
    "        logits = self.fc(dec_out)\n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z_BtHaU6j4Bn",
    "outputId": "18b659fc-20ae-4aa5-f319-c9cd5c0aa188"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "사용 중인 디바이스: cuda\n"
     ]
    }
   ],
   "source": [
    "# GPU 설정\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"사용 중인 디바이스: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RxIetCHWjhLW",
    "outputId": "147edc29-0f12-4ac9-8872-36bfe0e9a064"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어휘 크기: 4403\n"
     ]
    }
   ],
   "source": [
    "# VOCAB_SIZE 정의 (이전에 계산된 vocab_size 사용)\n",
    "VOCAB_SIZE = vocab_size  # 4409\n",
    "print(f\"어휘 크기: {VOCAB_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rRYPrTMGiq2q",
    "outputId": "08debc29-5860-4832-aec9-1828bc97978a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# 주어진 하이퍼파라미터로 Transformer 인스턴스 생성\n",
    "transformer = Transformer(\n",
    "    n_layers=2,\n",
    "    d_model=512,\n",
    "    n_heads=8,\n",
    "    d_ff=2048,\n",
    "    src_vocab_size=VOCAB_SIZE,\n",
    "    tgt_vocab_size=VOCAB_SIZE,\n",
    "    pos_len=200,\n",
    "    dropout=0.3,\n",
    "    shared_fc=True,\n",
    "    shared_emb=True)\n",
    "\n",
    "transformer = transformer.to(device)\n",
    "\n",
    "d_model = 512\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xPwLWiamivSv",
    "outputId": "2870db3d-23be-437e-a069-9a6c9ad2f4d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "class LearningRateScheduler:\n",
    "    def __init__(self, d_model, warmup_steps=60): # 4000\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        # step을 float으로 변환하여 지수 연산이 제대로 수행되도록 함\n",
    "        step = float(step)\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return (self.d_model ** -0.5) * min(arg1, arg2)\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BFqB_ltJiyiS",
    "outputId": "29a7d867-1584-4715-9234-97641bfd5816"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "# Learning Rate 인스턴스 선언\n",
    "learning_rate = LearningRateScheduler(d_model)\n",
    "\n",
    "# 초기 lr은 스텝 1에 해당하는 값으로 설정합니다.\n",
    "optimizer = torch.optim.Adam(transformer.parameters(),\n",
    "                             lr=learning_rate(1),\n",
    "                             betas=(0.9, 0.98),\n",
    "                             eps=1e-9)\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NEvNwyzni0Dc",
    "outputId": "e20874ec-1b8f-409f-a764-420e95ff8355"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def loss_function(real, pred):\n",
    "    \"\"\"\n",
    "    real: [batch_size, seq_len] (정답 토큰 인덱스)\n",
    "    pred: [batch_size, seq_len, num_classes] (모델의 raw logits)\n",
    "    \"\"\"\n",
    "\n",
    "    real = real.to(device)\n",
    "    pred = pred.to(device)\n",
    "\n",
    "    # 예측 값을 (N, C) 형태로 flatten하고, 정답도 flatten하여 개별 손실 값을 구함\n",
    "    loss_ = F.cross_entropy(pred.contiguous().view(-1, pred.size(-1)), real.contiguous().view(-1), reduction='none')\n",
    "    # 다시 (batch_size, seq_len)로 reshape\n",
    "    loss_ = loss_.view(real.size())\n",
    "\n",
    "    # real이 0이 아닌 위치에 대한 마스크 생성 (0이면 패딩 토큰)\n",
    "    mask = (real != 0).float()\n",
    "    loss_ = loss_ * mask\n",
    "\n",
    "    # 전체 손실 합을 마스크 합으로 나누어 평균 손실 계산\n",
    "    return loss_.sum() / mask.sum()\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dy3FLp0vi1iq",
    "outputId": "e0b6fe43-0268-481f-da24-83574a15eb14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "def train_step(src, tgt, model, optimizer):\n",
    "    model.train()  # 모델을 training 모드로 전환\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # tgt의 오른쪽 시프트: decoder input과 gold target 분리\n",
    "    tgt_in = tgt[:, :-1]  # Decoder의 입력\n",
    "    gold = tgt[:, 1:]     # Decoder의 정답(target)\n",
    "\n",
    "    # 마스크 생성 (generate_masks는 PyTorch용으로 변환된 함수여야 합니다)\n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_in)\n",
    "\n",
    "    src = src.to(device)\n",
    "    tgt_in = tgt_in.to(device)\n",
    "    enc_mask = enc_mask.to(device)\n",
    "    dec_enc_mask = dec_enc_mask.to(device)\n",
    "    dec_mask = dec_mask.to(device)\n",
    "\n",
    "    # 모델 forward pass\n",
    "    predictions, enc_attns, dec_attns, dec_enc_attns = model(src, tgt_in, enc_mask, dec_enc_mask, dec_mask)\n",
    "\n",
    "    # loss 계산\n",
    "    loss = loss_function(gold, predictions)\n",
    "\n",
    "    # 역전파 수행 및 파라미터 업데이트\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "BWNxn_5Jj9lJ"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HEOxg3kHkCJq",
    "outputId": "a014fa02-9299-4b1c-94bd-9ac0992bacd3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "데이터로더 생성 중...\n",
      "배치 수: 724\n",
      "총 데이터 수: 23151\n"
     ]
    }
   ],
   "source": [
    "# 데이터로더 생성\n",
    "def create_dataloader(enc_data, dec_data, batch_size=32, shuffle=True):\n",
    "    \"\"\"\n",
    "    PyTorch DataLoader 생성\n",
    "    \"\"\"\n",
    "    # numpy 배열을 torch tensor로 변환\n",
    "    enc_tensor = torch.tensor(enc_data, dtype=torch.long)\n",
    "    dec_tensor = torch.tensor(dec_data, dtype=torch.long)\n",
    "\n",
    "    # TensorDataset 생성\n",
    "    dataset = TensorDataset(enc_tensor, dec_tensor)\n",
    "\n",
    "    # DataLoader 생성\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "# 훈련 데이터로더 생성\n",
    "print(\"데이터로더 생성 중...\")\n",
    "train_dataloader = create_dataloader(enc_train, dec_train, batch_size=32, shuffle=True)\n",
    "print(f\"배치 수: {len(train_dataloader)}\")\n",
    "print(f\"총 데이터 수: {len(enc_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vd-fMFIfTO6G"
   },
   "source": [
    "### 6-2. 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a7toKztei2-7",
    "outputId": "eff8f457-c9ec-4e58-d13d-7dce8808f674"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 724/724 [00:26<00:00, 26.85it/s, Batch Loss=5243.5825]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 6702.0981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 724/724 [00:26<00:00, 27.57it/s, Batch Loss=4368.3027]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 4654.8520\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 724/724 [00:27<00:00, 26.76it/s, Batch Loss=3811.3582]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 3904.7675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 724/724 [00:26<00:00, 26.90it/s, Batch Loss=3131.9207]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 3425.8470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 724/724 [00:26<00:00, 27.19it/s, Batch Loss=2616.2566]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 3050.4474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 724/724 [00:26<00:00, 27.06it/s, Batch Loss=2637.8896]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 2772.7010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 724/724 [00:26<00:00, 27.00it/s, Batch Loss=2413.0388]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 2542.8039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 724/724 [00:26<00:00, 27.12it/s, Batch Loss=2123.8101]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 2337.9210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 724/724 [00:26<00:00, 26.95it/s, Batch Loss=1925.8085]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 2149.7097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 724/724 [00:26<00:00, 26.96it/s, Batch Loss=1746.8646]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 1976.6292\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 724/724 [00:26<00:00, 26.96it/s, Batch Loss=2045.7764]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Loss: 1815.5963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 724/724 [00:26<00:00, 27.06it/s, Batch Loss=1917.9896]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Loss: 1659.3727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 724/724 [00:26<00:00, 27.00it/s, Batch Loss=1522.4346]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Loss: 1515.9311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 724/724 [00:26<00:00, 27.06it/s, Batch Loss=1769.3717]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Loss: 1383.2143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 724/724 [00:26<00:00, 27.07it/s, Batch Loss=1151.5454]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Loss: 1262.9783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 724/724 [00:26<00:00, 26.97it/s, Batch Loss=1180.9697]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Loss: 1155.7820\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 724/724 [00:26<00:00, 26.83it/s, Batch Loss=982.1979]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Loss: 1060.9619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 724/724 [00:26<00:00, 27.07it/s, Batch Loss=913.6564]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Loss: 974.2169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 724/724 [00:26<00:00, 27.10it/s, Batch Loss=934.7592]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Loss: 900.8877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 724/724 [00:26<00:00, 27.09it/s, Batch Loss=803.2232]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Loss: 835.9125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 724/724 [00:26<00:00, 27.13it/s, Batch Loss=905.1976]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Loss: 782.5491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 724/724 [00:26<00:00, 27.09it/s, Batch Loss=958.3369]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Loss: 734.5266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 724/724 [00:26<00:00, 27.08it/s, Batch Loss=678.9667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Loss: 690.0462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 724/724 [00:26<00:00, 27.04it/s, Batch Loss=638.9466]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Loss: 653.8730\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 724/724 [00:26<00:00, 26.95it/s, Batch Loss=517.3447]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Loss: 621.8404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 724/724 [00:26<00:00, 26.92it/s, Batch Loss=599.3875]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Loss: 591.7822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 724/724 [00:26<00:00, 26.94it/s, Batch Loss=641.7620]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Loss: 565.7814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 724/724 [00:26<00:00, 26.97it/s, Batch Loss=683.9539]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Loss: 543.4885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 724/724 [00:26<00:00, 27.09it/s, Batch Loss=537.4249]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Loss: 520.8936\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 724/724 [00:26<00:00, 27.11it/s, Batch Loss=404.3753]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Loss: 501.7793\n",
      "CPU times: user 13min, sys: 6.86 s, total: 13min 7s\n",
      "Wall time: 13min 23s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "EPOCHS = 30\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0.0\n",
    "    dataset_count = len(train_dataloader)  # train_loader는 PyTorch DataLoader입니다.\n",
    "    tqdm_bar = tqdm(total=dataset_count)\n",
    "\n",
    "    for batch, (src, tgt) in enumerate(train_dataloader):\n",
    "        # train_step 함수는 (loss, enc_attns, dec_attns, dec_enc_attns)를 반환합니다.\n",
    "        loss, enc_attns, dec_attns, dec_enc_attns = train_step(src, tgt, transformer, optimizer)\n",
    "\n",
    "        total_loss += loss.item()  # PyTorch에서는 loss.numpy() 대신 loss.item() 사용\n",
    "        tqdm_bar.set_postfix({\"Batch Loss\": f\"{loss.item():.4f}\"})\n",
    "        tqdm_bar.update(1)\n",
    "\n",
    "    tqdm_bar.close()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss / dataset_count:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDJEQN4IJdE3"
   },
   "source": [
    "## Step 7. 성능 측정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "pfLxqYhKJe0B"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 최대 길이 상수 정의 (노트북의 max_length와 맞춤)\n",
    "MAX_LEN = 27  # 질문 최대 길이\n",
    "MAX_ANS_LEN = 31  # 답변 최대 길이"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "F5feJeG6e2rq"
   },
   "outputs": [],
   "source": [
    "def preprocess_input(sentence, sp_tokenizer, vocab_to_idx, max_length=27):\n",
    "    \"\"\"\n",
    "    입력 문장을 전처리하고 토큰화하여 모델 입력 형태로 변환\n",
    "\n",
    "    Args:\n",
    "        sentence: 입력 문장 (문자열)\n",
    "        sp_tokenizer: SentencePiece 토크나이저\n",
    "        vocab_to_idx: 어휘 사전 (토큰 -> 인덱스)\n",
    "        max_length: 최대 길이 (기본값: 27)\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: [1, max_length] 형태의 텐서 (배치 차원 포함)\n",
    "    \"\"\"\n",
    "    # 1. 전처리 (기존 preprocess_sentence 함수 사용)\n",
    "    processed = preprocess_sentence(sentence)\n",
    "\n",
    "    # 2. SentencePiece 토큰화\n",
    "    tokens = sp_tokenizer.encode_as_pieces(processed)\n",
    "\n",
    "    # 3. 인덱스로 변환\n",
    "    indices = []\n",
    "    for token in tokens:\n",
    "        if token in vocab_to_idx:\n",
    "            indices.append(vocab_to_idx[token])\n",
    "        else:\n",
    "            indices.append(vocab_to_idx[\"<unk>\"])\n",
    "\n",
    "    # 4. 패딩 또는 자르기\n",
    "    if len(indices) < max_length:\n",
    "        indices.extend([vocab_to_idx[\"<pad>\"]] * (max_length - len(indices)))\n",
    "    else:\n",
    "        indices = indices[:max_length]\n",
    "\n",
    "    return torch.tensor([indices], dtype=torch.long)  # 배치 차원 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "MOGnKg0BJkB_"
   },
   "outputs": [],
   "source": [
    "def translate(tokens, model, vocab_to_idx, idx_to_vocab, sp_tokenizer, device):\n",
    "    \"\"\"\n",
    "    주어진 토큰들로부터 응답을 생성하는 함수\n",
    "\n",
    "    Args:\n",
    "        tokens: 입력 토큰 리스트 (인덱스)\n",
    "        model: 훈련된 Transformer 모델\n",
    "        vocab_to_idx, idx_to_vocab: 어휘 사전\n",
    "        sp_tokenizer: SentencePiece 토크나이저\n",
    "        device: GPU/CPU 장치\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # tokens 길이 조정 (패딩 또는 자르기)\n",
    "    if len(tokens) > MAX_LEN:\n",
    "        tokens = tokens[:MAX_LEN]\n",
    "    else:\n",
    "        tokens = tokens + [vocab_to_idx[\"<pad>\"]] * (MAX_LEN - len(tokens))\n",
    "\n",
    "    # 배치 차원을 추가하여 텐서로 변환 (shape: [1, MAX_LEN])\n",
    "    padded_tokens = torch.tensor([tokens], dtype=torch.long, device=device)\n",
    "\n",
    "    ids = []\n",
    "    # 디코더의 첫 입력은 <start> 토큰 (배치 차원 추가)\n",
    "    output = torch.tensor([[vocab_to_idx[\"<start>\"]]], dtype=torch.long, device=device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(MAX_ANS_LEN - 1):  # <start> 제외하고 최대 길이만큼\n",
    "            # 마스크 생성\n",
    "            enc_mask, dec_enc_mask, dec_mask = generate_masks(padded_tokens, output)\n",
    "\n",
    "            # 모델 예측: predictions shape: [batch, seq_len, num_classes]\n",
    "            predictions, _, _, _ = model(padded_tokens, output, enc_mask, dec_enc_mask, dec_mask)\n",
    "\n",
    "            # 마지막 시퀀스 위치의 예측값을 소프트맥스 후 argmax로 선택\n",
    "            predicted_id = predictions[0, -1].softmax(dim=-1).argmax(dim=-1).item()\n",
    "\n",
    "            # <end> 토큰에 도달하면 현재까지의 예측 토큰 ids를 디코딩 후 반환\n",
    "            if predicted_id == vocab_to_idx[\"<end>\"]:\n",
    "                break\n",
    "\n",
    "            ids.append(predicted_id)\n",
    "\n",
    "            # 현재 output에 새로운 예측 토큰을 연결 (dim=1)\n",
    "            new_token = torch.tensor([[predicted_id]], dtype=torch.long, device=device)\n",
    "            output = torch.cat([output, new_token], dim=1)\n",
    "\n",
    "    # 토큰들을 텍스트로 변환\n",
    "    result_tokens = [idx_to_vocab[idx] for idx in ids if idx in idx_to_vocab]\n",
    "    result = sp_tokenizer.decode_pieces(result_tokens)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "N-MyfWK0JmTC"
   },
   "outputs": [],
   "source": [
    "def eval_bleu_single(model, src_sentence, tgt_sentence, vocab_to_idx, idx_to_vocab, sp_tokenizer, device, verbose=True):\n",
    "    \"\"\"\n",
    "    단일 문장 쌍에 대해 BLEU 점수 계산\n",
    "\n",
    "    Args:\n",
    "        src_sentence: 소스 문장 (문자열)\n",
    "        tgt_sentence: 타겟 문장 (문자열)\n",
    "    \"\"\"\n",
    "    # 소스 문장을 토큰화 (기존 preprocess_input 함수 활용)\n",
    "    src_tokens_tensor = preprocess_input(src_sentence, sp_tokenizer, vocab_to_idx, MAX_LEN)\n",
    "    src_tokens = src_tokens_tensor[0].tolist()  # 배치 차원 제거하고 리스트로 변환\n",
    "\n",
    "    # 타겟 문장을 토큰화\n",
    "    tgt_processed = preprocess_sentence(tgt_sentence)\n",
    "    tgt_tokens = sp_tokenizer.encode_as_pieces(tgt_processed)\n",
    "\n",
    "    # 길이 체크\n",
    "    if len([t for t in src_tokens if t != vocab_to_idx[\"<pad>\"]]) > MAX_LEN:\n",
    "        return None\n",
    "    if len(tgt_tokens) > MAX_ANS_LEN:\n",
    "        return None\n",
    "\n",
    "    # 참조 답변과 모델 예측 생성\n",
    "    reference = tgt_tokens  # 토큰화된 참조 답변\n",
    "    candidate_text = translate(src_tokens, model, vocab_to_idx, idx_to_vocab, sp_tokenizer, device)\n",
    "    candidate = sp_tokenizer.encode_as_pieces(candidate_text)  # 예측 결과를 토큰화\n",
    "\n",
    "    # BLEU 점수 계산\n",
    "    score = sentence_bleu([reference], candidate,\n",
    "                         smoothing_function=SmoothingFunction().method1)\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Source Sentence: \", src_sentence)\n",
    "        print(\"Model Prediction: \", candidate_text)\n",
    "        print(\"Model Tokens: \", candidate)\n",
    "        print(\"Real: \", tgt_sentence)\n",
    "        print(\"Real Tokens: \", reference)\n",
    "        print(\"Score: %f\\n\" % score)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "HyryjeKNJouP"
   },
   "outputs": [],
   "source": [
    "def eval_bleu(model, src_sentences, tgt_sentences, vocab_to_idx, idx_to_vocab, sp_tokenizer, device, verbose=True):\n",
    "    \"\"\"\n",
    "    여러 문장 쌍에 대해 전체 BLEU 점수 계산\n",
    "\n",
    "    Args:\n",
    "        src_sentences: 소스 문장들의 리스트\n",
    "        tgt_sentences: 타겟 문장들의 리스트\n",
    "    \"\"\"\n",
    "    total_score = 0.0\n",
    "    valid_samples = 0\n",
    "    sample_size = len(src_sentences)\n",
    "\n",
    "    print(f\"=== BLEU 평가 시작 (총 {sample_size}개 샘플) ===\")\n",
    "\n",
    "    for idx in tqdm(range(sample_size)):\n",
    "        try:\n",
    "            score = eval_bleu_single(\n",
    "                model, src_sentences[idx], tgt_sentences[idx],\n",
    "                vocab_to_idx, idx_to_vocab, sp_tokenizer, device, verbose=False\n",
    "            )\n",
    "            if score is not None:\n",
    "                total_score += score\n",
    "                valid_samples += 1\n",
    "\n",
    "            # 일부 샘플은 상세 출력\n",
    "            if verbose and idx < 5:\n",
    "                print(f\"\\n=== 샘플 {idx+1} ===\")\n",
    "                eval_bleu_single(\n",
    "                    model, src_sentences[idx], tgt_sentences[idx],\n",
    "                    vocab_to_idx, idx_to_vocab, sp_tokenizer, device, verbose=True\n",
    "                )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"샘플 {idx+1} 처리 중 오류: {e}\")\n",
    "            continue\n",
    "\n",
    "    if valid_samples > 0:\n",
    "        avg_score = total_score / valid_samples\n",
    "        print(f\"\\n=== BLEU 평가 결과 ===\")\n",
    "        print(f\"전체 샘플 수: {sample_size}\")\n",
    "        print(f\"유효 샘플 수: {valid_samples}\")\n",
    "        print(f\"평균 BLEU 점수: {avg_score:.6f}\")\n",
    "        return avg_score\n",
    "    else:\n",
    "        print(\"유효한 샘플이 없습니다.\")\n",
    "        return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "oNOuwg9BJrkw"
   },
   "outputs": [],
   "source": [
    "def evaluate_test_bleu(model, test_que, test_ans, vocab_to_idx, idx_to_vocab, sp_tokenizer, device, num_samples=None):\n",
    "    \"\"\"\n",
    "    테스트 데이터로 BLEU 점수 평가\n",
    "\n",
    "    Args:\n",
    "        test_que: 테스트 질문 리스트 (토큰화된 상태)\n",
    "        test_ans: 테스트 답변 리스트 (토큰화된 상태, <start>/<end> 포함)\n",
    "        num_samples: 평가할 샘플 수 (None이면 전체)\n",
    "    \"\"\"\n",
    "    # 토큰화된 데이터를 문자열로 변환\n",
    "    test_src_sentences = []\n",
    "    test_tgt_sentences = []\n",
    "\n",
    "    sample_count = len(test_que) if num_samples is None else min(num_samples, len(test_que))\n",
    "\n",
    "    for i in range(sample_count):\n",
    "        # 질문 토큰을 문자열로 변환\n",
    "        src_tokens = [idx_to_vocab[idx] for idx in test_que[i] if idx != vocab_to_idx[\"<pad>\"]]\n",
    "        src_sentence = sp_tokenizer.decode_pieces(src_tokens)\n",
    "        test_src_sentences.append(src_sentence)\n",
    "\n",
    "        # 답변 토큰을 문자열로 변환 (<start>, <end> 제거)\n",
    "        tgt_tokens = [idx_to_vocab[idx] for idx in test_ans[i]\n",
    "                     if idx not in [vocab_to_idx[\"<pad>\"], vocab_to_idx[\"<start>\"], vocab_to_idx[\"<end>\"]]]\n",
    "        tgt_sentence = sp_tokenizer.decode_pieces(tgt_tokens)\n",
    "        test_tgt_sentences.append(tgt_sentence)\n",
    "\n",
    "    # BLEU 평가 실행\n",
    "    bleu_score = eval_bleu(\n",
    "        model, test_src_sentences, test_tgt_sentences,\n",
    "        vocab_to_idx, idx_to_vocab, sp_tokenizer, device, verbose=True\n",
    "    )\n",
    "\n",
    "    return bleu_score\n",
    "    \"\"\"\n",
    "    입력 문장을 전처리하고 토큰화하여 모델 입력 형태로 변환\n",
    "    \"\"\"\n",
    "    # 1. 전처리 (기존 preprocess_sentence 함수 사용)\n",
    "    processed = preprocess_sentence(sentence)\n",
    "\n",
    "    # 2. SentencePiece 토큰화\n",
    "    tokens = sp_tokenizer.encode_as_pieces(processed)\n",
    "\n",
    "    # 3. 인덱스로 변환\n",
    "    indices = []\n",
    "    for token in tokens:\n",
    "        if token in vocab_to_idx:\n",
    "            indices.append(vocab_to_idx[token])\n",
    "        else:\n",
    "            indices.append(vocab_to_idx[\"<unk>\"])\n",
    "\n",
    "    # 4. 패딩 또는 자르기\n",
    "    if len(indices) < max_length:\n",
    "        indices.extend([vocab_to_idx[\"<pad>\"]] * (max_length - len(indices)))\n",
    "    else:\n",
    "        indices = indices[:max_length]\n",
    "\n",
    "    return torch.tensor([indices], dtype=torch.long)  # batch 차원 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "M3_t8od1JuWv"
   },
   "outputs": [],
   "source": [
    "def generate_response(model, input_sentence, sp_tokenizer, vocab_to_idx, idx_to_vocab,\n",
    "                     device, max_length=31, temperature=1.0):\n",
    "    \"\"\"\n",
    "    훈련된 모델을 사용해서 입력 문장에 대한 응답 생성\n",
    "\n",
    "    Args:\n",
    "        model: 훈련된 Transformer 모델\n",
    "        input_sentence: 입력 문장 (문자열)\n",
    "        sp_tokenizer: SentencePiece 토크나이저\n",
    "        vocab_to_idx, idx_to_vocab: 어휘 사전\n",
    "        device: GPU/CPU 장치\n",
    "        max_length: 최대 생성 길이\n",
    "        temperature: 생성 다양성 조절 (1.0=원본, 낮을수록 보수적)\n",
    "    \"\"\"\n",
    "    model.eval()  # 평가 모드로 전환\n",
    "\n",
    "    # 1. 입력 전처리\n",
    "    src = preprocess_input(input_sentence, sp_tokenizer, vocab_to_idx).to(device)\n",
    "\n",
    "    # 2. 디코더 입력 초기화 (<start> 토큰으로 시작)\n",
    "    tgt_input = torch.tensor([[vocab_to_idx[\"<start>\"]]], dtype=torch.long).to(device)\n",
    "\n",
    "    generated_tokens = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length - 1):  # <start> 제외하고 최대 길이만큼 생성\n",
    "            # 마스크 생성\n",
    "            enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt_input)\n",
    "\n",
    "            # 모델 예측\n",
    "            predictions, _, _, _ = model(src, tgt_input, enc_mask, dec_enc_mask, dec_mask)\n",
    "\n",
    "            # 마지막 토큰의 예측 결과 (다음 토큰 예측)\n",
    "            next_token_logits = predictions[0, -1, :]  # [vocab_size]\n",
    "\n",
    "            # Temperature 적용\n",
    "            if temperature != 1.0:\n",
    "                next_token_logits = next_token_logits / temperature\n",
    "\n",
    "            # 소프트맥스로 확률 분포 계산\n",
    "            probs = F.softmax(next_token_logits, dim=-1)\n",
    "\n",
    "            # 다음 토큰 샘플링 (확률적 선택)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "\n",
    "            # <end> 토큰이면 생성 종료\n",
    "            if next_token.item() == vocab_to_idx[\"<end>\"]:\n",
    "                break\n",
    "\n",
    "            # 생성된 토큰 저장\n",
    "            generated_tokens.append(next_token.item())\n",
    "\n",
    "            # 디코더 입력에 새 토큰 추가\n",
    "            tgt_input = torch.cat([tgt_input, next_token.unsqueeze(0)], dim=1)\n",
    "\n",
    "    # 3. 토큰을 텍스트로 변환\n",
    "    response_tokens = [idx_to_vocab[idx] for idx in generated_tokens if idx in idx_to_vocab]\n",
    "    response_text = sp_tokenizer.decode_pieces(response_tokens)\n",
    "\n",
    "    return response_text, response_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "tAYE6n64JyZz"
   },
   "outputs": [],
   "source": [
    "# 사용 예시\n",
    "def test_model_responses(model, sp_tokenizer, vocab_to_idx, idx_to_vocab, device,\n",
    "                        reference_answers=None):\n",
    "    \"\"\"\n",
    "    몇 가지 테스트 문장으로 모델 성능 확인 (BLEU 점수 포함)\n",
    "\n",
    "    Args:\n",
    "        reference_answers: 각 테스트 문장에 대한 참조 답변 리스트 (선택사항)\n",
    "    \"\"\"\n",
    "    from nltk.translate.bleu_score import sentence_bleu\n",
    "    from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "    test_sentences = [\n",
    "        \"사랑이란 뭘까?\"\n",
    "        \"안녕하세요\",\n",
    "        \"오늘 날씨가 좋네요\",\n",
    "        \"여행 가고 싶어요\",\n",
    "        \"심심해요\",\n",
    "        \"고마워요\",\n",
    "        \"배고파요\",\n",
    "        \"영화 추천해주세요\",\n",
    "        \"기분이 안 좋아요\"\n",
    "    ]\n",
    "\n",
    "    # 기본 참조 답변 (실제 데이터에서 가져오거나 수동으로 작성)\n",
    "    if reference_answers is None:\n",
    "        reference_answers = [\n",
    "            [\"사랑에는\", \"답이\", \"없어요\"],\n",
    "            [\"안녕하세요\", \"반갑습니다\", \"안녕\"],\n",
    "            [\"날씨가 정말 좋네요\", \"좋은 날씨네요\", \"날씨 좋아요\"],\n",
    "            [\"여행 좋죠\", \"어디로 가고 싶으세요\", \"여행은 언제나 좋죠\"],\n",
    "            [\"뭔가 재미있는 걸 해보세요\", \"심심하시군요\", \"재미있는 일이 있으면 좋겠네요\"],\n",
    "            [\"천만에요\", \"별말씀을요\", \"도움이 되어서 다행이에요\"],\n",
    "            [\"맛있는 음식 드세요\", \"뭔가 드시면 좋겠네요\", \"배고프시군요\"],\n",
    "            [\"좋은 영화 추천해드릴게요\", \"어떤 장르 좋아하세요\", \"영화 보는 거 좋죠\"],\n",
    "            [\"위로해 드립니다\", \"기분이 나아지면 좋겠어요\", \"힘내세요\"]\n",
    "        ]\n",
    "\n",
    "    print(\"=== 모델 테스트 (BLEU 점수 포함) ===\\n\")\n",
    "\n",
    "    total_bleu_scores = []\n",
    "    smoothie = SmoothingFunction().method4  # 스무딩 함수\n",
    "\n",
    "    for i, sentence in enumerate(test_sentences):\n",
    "        try:\n",
    "            response, tokens = generate_response(\n",
    "                model, sentence, sp_tokenizer, vocab_to_idx,\n",
    "                idx_to_vocab, device, temperature=0.7\n",
    "            )\n",
    "\n",
    "            # 생성된 응답을 토큰화 (BLEU 계산용)\n",
    "            generated_tokens = sp_tokenizer.encode_as_pieces(response)\n",
    "\n",
    "            # 참조 답변들을 토큰화\n",
    "            reference_tokens_list = []\n",
    "            for ref in reference_answers[i]:\n",
    "                ref_tokens = sp_tokenizer.encode_as_pieces(ref)\n",
    "                reference_tokens_list.append(ref_tokens)\n",
    "\n",
    "            # BLEU 점수 계산 (1-gram부터 4-gram까지)\n",
    "            bleu_1 = sentence_bleu(reference_tokens_list, generated_tokens,\n",
    "                                 weights=(1, 0, 0, 0), smoothing_function=smoothie)\n",
    "            bleu_2 = sentence_bleu(reference_tokens_list, generated_tokens,\n",
    "                                 weights=(0.5, 0.5, 0, 0), smoothing_function=smoothie)\n",
    "            bleu_3 = sentence_bleu(reference_tokens_list, generated_tokens,\n",
    "                                 weights=(0.33, 0.33, 0.33, 0), smoothing_function=smoothie)\n",
    "            bleu_4 = sentence_bleu(reference_tokens_list, generated_tokens,\n",
    "                                 weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smoothie)\n",
    "\n",
    "            total_bleu_scores.append([bleu_1, bleu_2, bleu_3, bleu_4])\n",
    "\n",
    "            print(f\"입력: {sentence}\")\n",
    "            print(f\"출력: {response}\")\n",
    "            print(f\"BLEU-1: {bleu_1:.4f}\")\n",
    "            print(f\"BLEU-2: {bleu_2:.4f}\")\n",
    "            print(f\"BLEU-3: {bleu_3:.4f}\")\n",
    "            print(f\"BLEU-4: {bleu_4:.4f}\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"'{sentence}' 처리 중 오류: {e}\")\n",
    "            print(\"-\" * 50)\n",
    "\n",
    "    # 전체 평균 BLEU 점수 계산\n",
    "    if total_bleu_scores:\n",
    "        avg_bleu_scores = np.mean(total_bleu_scores, axis=0)\n",
    "        print(f\"\\n=== 평균 BLEU 점수 ===\")\n",
    "        print(f\"평균 BLEU-1: {avg_bleu_scores[0]:.4f}\")\n",
    "        print(f\"평균 BLEU-2: {avg_bleu_scores[1]:.4f}\")\n",
    "        print(f\"평균 BLEU-3: {avg_bleu_scores[2]:.4f}\")\n",
    "        print(f\"평균 BLEU-4: {avg_bleu_scores[3]:.4f}\")\n",
    "\n",
    "        return avg_bleu_scores\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iRQouFx9J0bB",
    "outputId": "b4240dce-98d7-4c90-a20f-0a03f372c5bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 모델 테스트 (BLEU 점수 포함) ===\n",
      "\n",
      "입력: 사랑이란 뭘까?안녕하세요\n",
      "출력: 당신도 잘 여 기 전에 무엇 에서 시작 이네요 .\n",
      "BLEU-1: 0.0000\n",
      "BLEU-2: 0.0000\n",
      "BLEU-3: 0.0000\n",
      "BLEU-4: 0.0000\n",
      "--------------------------------------------------\n",
      "입력: 오늘 날씨가 좋네요\n",
      "출력: 잘 챙겨 아는 것도 좋아요 .\n",
      "BLEU-1: 0.0000\n",
      "BLEU-2: 0.0000\n",
      "BLEU-3: 0.0000\n",
      "BLEU-4: 0.0000\n",
      "--------------------------------------------------\n",
      "입력: 여행 가고 싶어요\n",
      "출력: 저도 좋아해요 .\n",
      "BLEU-1: 0.0000\n",
      "BLEU-2: 0.0000\n",
      "BLEU-3: 0.0000\n",
      "BLEU-4: 0.0000\n",
      "--------------------------------------------------\n",
      "입력: 심심해요\n",
      "출력: 잘 버텨 이네요 .\n",
      "BLEU-1: 0.0000\n",
      "BLEU-2: 0.0000\n",
      "BLEU-3: 0.0000\n",
      "BLEU-4: 0.0000\n",
      "--------------------------------------------------\n",
      "입력: 고마워요\n",
      "출력: 잘 챙겨 드세요 .\n",
      "BLEU-1: 0.0000\n",
      "BLEU-2: 0.0000\n",
      "BLEU-3: 0.0000\n",
      "BLEU-4: 0.0000\n",
      "--------------------------------------------------\n",
      "입력: 배고파요\n",
      "출력: 잘 챙겨 드세요 .\n",
      "BLEU-1: 0.0000\n",
      "BLEU-2: 0.0000\n",
      "BLEU-3: 0.0000\n",
      "BLEU-4: 0.0000\n",
      "--------------------------------------------------\n",
      "입력: 영화 추천해주세요\n",
      "출력: 영화 우 책 는 영화 보고 싶은 보고 싶은 보고 싶은 보고 싶은\n",
      "BLEU-1: 0.0000\n",
      "BLEU-2: 0.0000\n",
      "BLEU-3: 0.0000\n",
      "BLEU-4: 0.0000\n",
      "--------------------------------------------------\n",
      "입력: 기분이 안 좋아요\n",
      "출력: 잘 챙겨 드세요 .\n",
      "BLEU-1: 0.0000\n",
      "BLEU-2: 0.0000\n",
      "BLEU-3: 0.0000\n",
      "BLEU-4: 0.0000\n",
      "--------------------------------------------------\n",
      "\n",
      "=== 평균 BLEU 점수 ===\n",
      "평균 BLEU-1: 0.0000\n",
      "평균 BLEU-2: 0.0000\n",
      "평균 BLEU-3: 0.0000\n",
      "평균 BLEU-4: 0.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0.])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델 테스트\n",
    "test_model_responses(transformer, sp, vocab_to_idx, idx_to_vocab, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iRQouFx9J0bB",
    "outputId": "b4240dce-98d7-4c90-a20f-0a03f372c5bf"
   },
   "source": [
    "### 회고\n",
    "\n",
    "1. 동작은 하나 자연스러운 답이 나오지 않음.\n",
    "2. 증강이 오히려 성능을 떨어트리는 것 같음. 약간의 노이즈는 도움이 되나 노이즈가 너무 큰것 같음.\n",
    "3. 챗봇 등 생성 모델에는 BLEU score가 좋은 척도는 아니라 생각됨. 왜냐면 정답이 정해져 있지 않기 때문에"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
